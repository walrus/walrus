\documentclass[a4paper]{article}

% Table of contents depth
\setcounter{tocdepth}{3}

% Section numbering depth (zero for no numbering)
\setcounter{secnumdepth}{0}

% LaTeX package inclusions
\usepackage[english]{babel}

\usepackage{fullpage} % Page width options

\usepackage{hyperref} % Internal and external references
\usepackage{url}      % Unused?
\usepackage{breakurl} % Support for sensible line breaks in URLS

\usepackage{tabulary} % Fun with tables
\usepackage{float}    % Allows for better placement of tables etc
\usepackage{array}    % More options for tables
\usepackage{multirow} % Support for multi row columns in tables

\usepackage{graphicx} % For picture inclusion
\graphicspath{{images/}}

\usepackage[colorinlistoftodos]{todonotes} % For the inclusion of TODOs
\usepackage[toc,page]{appendix} % Generation of bibliography/appendix

% Source code inclusion
\usepackage{listings}
\lstset{
  tabsize=2,
  basicstyle = \ttfamily\small,
  columns=fullflexible
}
% Usage for the above like so:
% \begin{lstlisting}
%   CODE CODE CODE
% \end{lstlisting}

% In-line code styling (same style as listing)
\newcommand{\shell}[1]{\lstinline{#1}}

% Use roman numerals for page numbers in the contents
\pagenumbering{roman}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{\textbf{WALRUS:}\\
Exercise classification on embedded hardware}
\date{2017}
\author{
Daniel Clay\\
\emph{Supervised by Dr Thomas Heinis}\\ 
}
\maketitle
\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{Abstract: do this last, goes the advice}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{Acknowledgements: Dr Heinis et al}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Table of Contents on new page
\pagebreak
\tableofcontents
\pagebreak

% Use arabic numerals after contents
\pagenumbering{arabic}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Motivation}%%%%%%%%%%%%%%%%%%%%%%%%%




In the West, obesity is a growing problem - as of 2015, \textbf{62.9\%} of adults in the UK were either overweight or obese, a number which is still increasing.\cite{inref1}

While many people express a desire to exercise more this does not always translate into action, and one of the reasons for this is that
people often don't know how to exercise, and even if they do exercise this lack of knowledge stops or hampers them from exercising as effectively as they might be able to.

Traditionally the solution to this dilemma has been provided by people like coaches, instructors and personal trainers. They generally bring a lot of expertise and experience, and are highly effective, but the cost of hiring them can be unaffordable for many people.

With advances in technology, more and more of the tasks which personal trainers and their ilk were relied upon to do can instead be performed by machines. Companies are starting to release gadgets which can classify different exercises, normally into large groups such as 'walking' or 'cycling', but as yet they do not tell the user how well they are performing said exercise.

\subsection{Issues}%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{What makes this difficult}

\subsection{Contributions}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

My contributions for this project will be split broadly into two areas: a neural network library, and an exercise classifier using the library.

A more specific breakdown of the contributions is as follows:

\begin{itemize}
\item Neural Network Library 
  \begin{itemize}
  \item Implementation of a neural network capable of running on  both desktop and microcontroller
    \item Storage and retrieval of network configurations
  \end{itemize}
\item Exercise Classifier
  \begin{itemize}
  \item Programs to acquire training data
    \item A trained network capable of reliably classifying the three chosen exercises
    \item An implementation of a classifier using the above network, running on a microcontroller
    \item An implementation of the exercise classifier making use of more than one microcontroller (extension)
  \end{itemize}
\item A data set to train the exercise classifier on
\end{itemize}

In addition to checking the completion of each contribution, the success of the project can be measured in terms of how \textit{capable} and \textit{efficient} the resulting contributions are. 

Capability is the measure of how many different tasks a system can perform.
Efficiency is the measure of how well the system performs the tasks of which it is capable. 

The success of the various contributions can be judged by how well they help the project to meet these criteria.

\subsubsection{Neural Network Evaluation Metrics}

For the library, capability means that it must be able to do the following:

\begin{itemize}
\item Create an arbitrarily sized network (with one hidden layer)
\item Run on both desktop and embedded systems
\item Be capable of storing and retrieving configurations from disk
\item Be capable of implementing all the functionality of ArduinoANN
\end{itemize}

For the library, efficiency means the following:

\begin{itemize}
\item It must use as little memory as possible
\item It must use as little processing power as possible
\item It must be as easy to use as possible
\item It must have as few dependencies as possible
\end{itemize}

\subsubsection{Exercise Classifier Evaluation Metrics}

For the classifier, capability means that it must be able to do the following:

\begin{itemize}
\item Classify the user's movements \textit{in real time}, or close to it
\item Connect over Bluetooth to a phone
\item Display the classifications
\end{itemize}

For the classifier, efficiency means the following:

\begin{itemize}
\item It must classify the movements reliably and correctly
\item It must use as little memory as possible
\item It must use as little processing power as possible
\item It must be as easy to use as possible
\end{itemize}

\subsubsection{Data Set Evaluation Metrics}

For the data set, capability means that it must have the following:

\begin{itemize}
\item Correctly labelled data for all of the classifications
\item Data of the correct format for consumption by the neural network
\end{itemize}

For the data set, efficiency means the following:

\begin{itemize}
\item It must have as little noise as possible
\item It must have as few misclassified examples as possible
\item It must have the correct balance of examples so as to maximise the efficiency of the classifier
\item It must have enough examples to train the classifier effectively
\end{itemize}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Neural Networks}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Neural networks are currently the focus of a lot of research in academia, and also the focus of a lot of commercial work to realise their potential - virtually all of the major tech companies make use of the technology, or are experimenting with it.

Because my project is centered around a novel usage of existing neural network techniques, rather than any new techniques, my research into the theory of neural networks has been relatively limited, and I have focussed on their application to my project.

Most of my general knowledge of Neural Networks comes from the third year DoC course \textbf{395 Machine Learning}\cite{bgref0}. (in particular, see the notes for the Artificial Neural Networks lectures):

As well as the course notes, the lecturers have recommended several other resources (a list of which can be found on the previously linked page), of which the paper \textit{Artifical Neural Networks: A Tutorial}\cite{bgref1} is the most relevant. This provided an overall introduction to neural networks and gave a different perspective to that given by the DoC course.

Another useful reference has been \textit{Neural Networks - Algorithms and Applications}\cite{bgref2}. This contains lots of practical information of direct relevance to the implementation of neural networks.

\subsection{ArduinoANN}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

My neural network implementation is based on \textit{ArduinoANN}\cite{bgref3}, a C implementation of a two layer feed-forward backpropagation network. 

ArduinoANN itself draws heavily on \textit{John Bullinaria's Step by Step Guide to Implementing a Neural Network in C}\cite{bgref4}.

\subsection{Embedded Systems \& The Intel Curie}%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Embedded Systems}

While most machine learning has historically done on relatively powerful desktop systems or specialised hardware, modern embedded systems are often more powerful than the desktop computers available when techniques such as back propagation were developed.\cite{bgref5}. 

As embedded systems become more powerful and more widespread, machine learning programs will be run on them much more often.

\subsubsection{The Intel Curie}

The Intel Curie is a System-On-a-Chip (SOC) with an integrated six-axis accelerometer/gyroscope and an integrated 128-neuron neural network that operates on the data from the sensors.

Despite the presence of the neural network, which Intel call the \textit{Curie Pattern Matching Engine}\cite{bgref6}, I will be implementing a separate neural network. This decision is explained in another section. \todo[inline]{link when written}

The module is currently available on an Arduino 101 board (known as the Genuino 101 outside the US), or an Intel Quark microcontroller - I will be developing using the Arduino board.

The Arduino/Genuino 101 provides a set of I/O pins, power supply, and Bluetooth connectivity for the on-board Curie module.

More information can be found at Intel's homepage for the Curie\cite{bgref6}.

Information on software dependencies can be found here. \todo[inline]{link when done}

\subsection{Bluetooth / Bluetooth Low Energy (BLE)}%%%%%%%%%%%%%%%%%%%%%%%

Bluetooth Low Energy\cite{bgref7} is a low power version of the Bluetooth standard designed to allow devices to transfer small amounts of data in an energy efficient manner. The standard debuted in 2011, and is now widely supported by Internet of Things (IoT) and mobile devices.

\subsection{Fitness Trackers}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There are a wide variety of fitness trackers available on the market - most utilise a combination of gyroscopes, accelerometers and GPS, and more sometimes heart rate monitors, altimeters and other sensors to track the user's movements and provide feedback.

Tracking via sensors other than those available on the Intel Curie (I.E. gyroscope and accelerometer) is outside the scope of this project, so I will be ignoring this functionality in other fitness trackers.

Of these, the majority simply measure the input data and do simple analysis to provide basic data, E.G. heart rate or calories burned, but a growing number use various forms of machine learning to do more in depth analysis. 

\subsubsection{Fitbit}

Fitbit is one of the leading brands in wearables and fitness tracking. In particular, their \textit{SmartTrack}\cite{bgref8} technology aims to automatically recognise which activity the wearer is currently doing, and currently supports seven different activities.

\subsubsection{Google}

Google has a line of wearables called \textit{Android Wear}\cite{bgref9}, which make use of their \textit{Google Fit}\cite{bgref10} software (also compatible with other wearables).
Google Fit automatically detects walking, running and cycling.

\subsubsection{Optimize Fitness}

Optimize Fitness\cite{bgref11} is an iOS application that claims to use 'Powerful machine learning algorithms' to 'analyze your preferences, workout history, and goals to deliver efficient workouts that keep you improving wherever and whenever you exercise.'

\subsubsection{Boltt}

Boltt\cite{bgref12} is a startup that aims to use multiple sensors (embedded in shoes and on a wristband) along with AI to give guidance. Currently it is in the pre-order stage.

\subsubsection{Actofit}

Actofit\cite{bgref13} is another startup that recently funded via Indiegogo\cite{bgref14}. They claim to 'identify 75+ exercises, count reps, evaluate form, measure heart rate, calories burned and more' using a wristband.

\subsubsection{FocusMotion}

FocusMotion\cite{bgref15} provides an SDK that works on many different devices, and uses their sensors as input to their machine learning algorithms, which aim to classify and analyse user's movements.

\subsection{Bodyweight Exercises}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While there is no objective standard for how most exercises should be performed, for the most common exercises there exists a broad consensus on the proper technique.

I will use these as a baseline to compare a user's movements to, and to ensure that when recording training data the examples are correct. I have used the following guides as my references.

\subsubsection{Press Ups}

The generally agreed proper technique for a press up is as follows:

\begin{itemize}
    \item Place your hands on the ground slightly more than shoulder width apart, and your feet behind you. Your body should be straight, with no sagging at the hips. You should be looking slightly ahead, not vertically downwards, and your arms should be locked out.
    \item Keeping your body straight, lower yourself down until your elbows are at a 90° angle to the floor, and upper arms are horizontal. Your elbows should remain close in to the side of your body, not splayed out to the sides.
    \item In the same manner, raise yourself back up with your arms until they lock out again.
\end{itemize}

I have used the following two articles as my reference for good technique when doing a press up:

Nerd Fitness' article \textit{How to do a Proper Push Up}\cite{bgref16}
is a good overall guide to technique, while Breaking Muscle's article \textit{Pimp Your Push Up: 3 Common Mistakes And 5 Challenging Variations}\cite{bgref17} addresses a few common issues with people's technique.

A video illustrating these points can be found on Youtube.\cite{bgref18}

\subsubsection{Sit Ups}

Opinions on proper sit up technique vary, primarily over the placement of the arms - some guides recommend crossing them over your chest, while others recommend placing your hands on the back of your head. 

LiveStrong.com\cite{bgref19} recommends placing hands behind the head, while military.com\cite{bgref20}recommends crossing your arms over your chest. 

The general opinion seems to be that either is acceptable, with the hands-behind-the-head technique considered slightly harder\cite{bgref21}.

Keeping your hands loose is generally considered a bad thing as it allows you to use them to lift your torso, rather than your abdominal muscles. 

For my reference, I will use the crossed arms technique because placing your arms behind your head encourages you to pull yourself up from the neck, rather than the waist, which the crossed arms technique avoids.

There is also not a consensus on whether it is better to anchor your feet during a sit up. For the purposes of my reference, I will be recommending unanchored feet. 

For more specific information on unanchored sit ups, the article \textit{How to Do Sit-Ups Without Anchoring Your Feet}\cite{bgref22}
The proper technique for a sit up, with the caveats above, is as follows:

\begin{itemize}
    \item Lie flat on your back, with your knees bent at a 90° angle and feet on the floor. Cross your arms over your chest and straighten your neck. and spine.
    \item Keeping your legs immobile, lift your back off the floor by flexing at the waist, and continue until your back is vertical. This should be a smooth, controlled movement not a jerk, and should not be assisted by the arms. Your neck should remain straight, but the spine can flex a little. You may find it helpful to exhale as you do this.
    \item Having reached the upright position, rest if necessary and lower yourself back down in the same manner. You may find it helpful to inhale as you do this.
\end{itemize}

This video\cite{bgref23} gives a good demonstration of good sit up technique, although it uses the hands behind the head technique.

\subsubsection{Lunges}

Opinions on proper lunge technique are fairly settled, with a strong consensus. While there are many possible varieties of lunges, including those with weights, I will concentrate on lunges using bodyweight alone.

My main source for technique was Shape.com\cite{bgref24}.

My baseline reference for good lunge technique is as follows:

\begin{itemize}
    \item Stand up straight, with shoulders relaxed and core engaged
    \item Step forwards with one leg, and lower your body until the forward knees is bent at 90° to the floor. The back knee should not touch the floor, and your upper body should remain upright.
    \item In the same manner, smoothly push back up to your starting position.
\end{itemize}

This video\cite{bgref25} illustrates good lunge technique.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design \& Specification - Neural Network Library}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

I will be creating a C++ class-based library to facilitate the implementation of two layer feed-forward backpropagation neural networks on embedded systems, based on \todo[inline]{ArduinoANN (link to background).}

The library is designed to enable a network to be trained on a (more powerful) desktop system, and then uploaded to the embedded system for use.

Because of this, there are two core Network classes: \lstinline{Network_L} (which can be found in the file \lstinline{network-linux.hpp}) supports training and makes more extensive use of RAM, while \lstinline{Network_A} (found in the file \lstinline{network-arduino.hpp} cannot be trained and is designed to minimise RAM usage, instead storing as much data as possible in Program Memory. If the network is small enough, and an implementation of the STL is available, \lstinline{Network_L} can be run on a microcontroller.

Functions for writing and reading network configurations to and from disk can be found in the file \lstinline{network-saveload-linux.hpp}

\subsection{Language \& Dependencies}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

All three files are written to conform to the C++11 standard\cite{nnref0}. The two Linux-based files further depend on the C++ Standard Template Library, in particular the \lstinline{std::vector} data structure.

The Arduino file lacks the dependency on the STL, but itself depends on the \textit{pgmspace} library.\cite{nnref1}

\subsection{API Reference}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Network\_L API}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting} 
Network_L(int numInputNodes,
          int numHiddenNodes,
          int numOutputNodes,
          float learningRate,
          float momentum,
          float initialWeightMax,
          long trainingCycle);
\end{lstlisting}

\textbf{Description: }
Constructs a new instance of \lstinline{Network_L} in memory, initialising all nodes to zero and all weights to \lstinline{|weight| < initialWeightMax}

\textbf{Parameters: }

\lstinline{numInputNodes} The number of nodes in the input layer. Cannot be changed after initialisation.

\lstinline{numHiddenNodes} The number of nodes in the hidden layer. Cannot be changed after initialisation.

\lstinline{numOutputNodes} The number of nodes in the output layer. Cannot be changed after initialisation.

\lstinline{learningRate} The factor by which the backpropagated error is multiplied. A lower learning rate results in slower learning, but is less prone to over correction and oscillating weight changes. The learning rate should be in the range \lstinline{0 < learningRate < 1}.

\lstinline{momentum} The momentum term, which helps smooth out weight changes and avoid local minima. Must be in the range \lstinline{0 =< momentum < 1}. Setting momentum to zero is the same as backpropagation without momentum.

\lstinline{initialWeightMax} Defines the maximum absolute value of weights upon initialisation. All weights will be initialised to \lstinline{|weight| < initialWeightMax} 

\textbf{Returns}
A pointer to the newly initialised \lstinline{Network_L}.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float trainNetwork(std::vector<float> inputs,
                   std::vector<float> targets);
\end{lstlisting}

\textbf{Description: }
Trains the network on the given sets of inputs and targets using backpropagation.

\textbf{Parameters: }

\lstinline{inputs} The set of inputs to the network. Must contain \lstinline{numInputNodes} elements.

\lstinline{targets} The set of targets for the network to compare output to. Must contain \lstinline{numOutputNodes} elements.

\textbf{Returns: }
The cumulative error of the network before applying backpropagation (the sum of the differences between the outputs and the targets).

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
std::string writeReport();
\end{lstlisting}

\textbf{Description: }
Returns information about the current state of the network for display purposes.

\textbf{Parameters: } None

\textbf{Returns: }
A string which contains the current training cycle and error rate of the network.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
std::vector<float> classify(std::vector<float> inputs);
\end{lstlisting}

\textbf{Description: }
Attempts to classify the given set of inputs using the network. Unlike \lstinline{trainNetwork()}, will not modify the state of the network.

\textbf{Parameters: }

\lstinline{inputs} The set of inputs to the network. Must contain \lstinline{numInputNodes} elements.

\textbf{Returns: }
A vector containing a copy of the values of the output nodes after classification.
\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
void loadWeights(std::vector<std::vector<float>> hiddenWeights,
                 std::vector<std::vector<float>> outputWeights);
\end{lstlisting}

\textbf{Description: }
Replaces the network's weights with the values in the given vectors.

\textbf{Parameters: }

\lstinline{hiddenWeights} An \lstinline{m} x \lstinline{n} vector of vectors containing the values to replace the hidden weights with, where \lstinline{m = numInputNodes + 1} and \lstinline{n = numHiddenNodes}.

\lstinline{outputWeights} An \lstinline{m} x \lstinline{n} vector of vectors containing the values to replace the output weights with, where \lstinline{m = numHiddenNodes + 1} and \lstinline{n = numOutputNodes}.

\textbf{Returns: } Void.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
int getNumInputNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{numInputNodes}

\textbf{Parameters: } None

\textbf{Returns: }
The number nodes in the input layer.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
int getNumHiddenNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{numHiddenNodes}

\textbf{Parameters: } None

\textbf{Returns: }
The number nodes in the hidden layer.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
int getNumOutputNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{numOutputNodes}

\textbf{Parameters: } None

\textbf{Returns: }
The number nodes in the output layer.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getLearningRate() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{learningRate}

\textbf{Parameters: } None

\textbf{Returns: }
The current network learning rate.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getMomentum() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{momentum}

\textbf{Parameters: } None

\textbf{Returns: }
The current network momentum.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getInitialWeightMax() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{initialWeightMax}

\textbf{Parameters: } None

\textbf{Returns: }
The initial weight maximum value.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
long getTrainingCycle() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{trainingCycle}

\textbf{Parameters: } None

\textbf{Returns: }
The current training cycle

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getRandomFloat() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{randomFloat}

\textbf{Parameters: } None

\textbf{Returns: }
The current value of \lstinline{randomFloat}, which will be in the range \lstinline{-1.0 < randomFloat < 1.0}

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getErrorRate() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{errorRate}

\textbf{Parameters: } None

\textbf{Returns: }
The current value of the network error rate.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getAccumulatedInput() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{accumulatedInput}

\textbf{Parameters: } None

\textbf{Returns: }
The current value of the network's accumulated input.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const std::vector<float> getHiddenNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{hiddenNodes}

\textbf{Parameters: } None

\textbf{Returns: }
A \textit{copy} of the current value of the network's hidden nodes.
\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const std::vector<float> getOutputNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{outputNodes}

\textbf{Parameters: } None

\textbf{Returns: }
A \textit{copy} of the current value of the network's output nodes.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const std::vector<float> getHiddenNodesDeltas() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{hiddenNodesDeltas}, the magnitude of the difference between the target and the actual outputs for each hidden node.

\textbf{Parameters: } None

\textbf{Returns: }
A \textit{copy} of the current value of the network's hidden node deltas,

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const std::vector<float> getOutputNodesDeltas() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{outputNodesDeltas}, the magnitude of the difference between the target and the actual outputs for each output node.

\textbf{Parameters: } None

\textbf{Returns: }
A \textit{copy} of the current value of the network's output node deltas.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const std::vector<std::vector<float>> getHiddenWeights() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{hiddenWeights}

\textbf{Parameters: } None

\textbf{Returns: }
A \textit{copy} of the current value of the network's hidden weights.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const std::vector<std::vector<float>> getOutputWeights() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{outputWeights}

\textbf{Parameters: } None

\textbf{Returns: }
A \textit{copy} of the current value of the network's output weights.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const std::vector<std::vector<float>> getHiddenWeightsChanges() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{hiddenWeightsChanges}, the change to be applied to each hidden weight to compensate for it's error. Depending on when this function is called, the change may already have been applied.

\textbf{Parameters: } None

\textbf{Returns: }
A \textit{copy} of the current value of the network's hidden weights changes.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const std::vector<std::vector<float>> getOutputWeightsChanges() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{outputWeightsChanges}, the change to be applied to each output weight to compensate for it's error. Depending on when this function is called, the change may already have been applied.

\textbf{Parameters: } None

\textbf{Returns: }
A \textit{copy} of the current value of the network's hidden weights changes.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
void setLearningRate(float learningRate);
\end{lstlisting}

\textbf{Description: }
Setter for \lstinline{learningRate}

\textbf{Parameters: }

\lstinline{learningRate} The new value to set the network's learning rate to.

\textbf{Returns: } Void

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
void setMomentum(float momentum);
\end{lstlisting}

\textbf{Description: }
Setter for \lstinline{momentum}

\textbf{Parameters: }

\lstinline{learningRate} The new value to set the network's momentum to.

\textbf{Returns: } Void

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Network\_A API}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
Network_A();
\end{lstlisting}

\textbf{Description: }
Constructs a new instance of \lstinline{Network_A}, with the nodes in RAM. Relies on the presence of the network configuration in program memory.

\textbf{Parameters: } None

\textbf{Returns: }
A pointer to the newly initialised \lstinline{Network_A}.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
std::string writeReport();
\end{lstlisting}

\textbf{Description: }
Returns information about the current state of the network for display purposes.

\textbf{Parameters: } None

\textbf{Returns: }
A string which contains the current training cycle and error rate of the network.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float * classify(float inputs[]);
\end{lstlisting}
\textbf{Description: }
Attempts to classify the given set of inputs using the network.

\textbf{Parameters: }

\lstinline{inputs} The set of inputs to the network. Must contain \lstinline{numInputNodes} elements.

\textbf{Returns: }
A pointer to the array containing the values of the output nodes after classification. 
\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
int getNumInputNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{numInputNodes}

\textbf{Parameters: } None

\textbf{Returns: }
The number of nodes in the input layer.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
int getNumHiddenNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{numHiddenNodes}

\textbf{Parameters: } None

\textbf{Returns: }
The number of nodes in the hidden layer.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
int getNumOutputNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{numOutputNodes}

\textbf{Parameters: } None

\textbf{Returns: }
The number of nodes in the output layer.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getLearningRate() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{learningRate}

\textbf{Parameters: } None

\textbf{Returns: }
The current network learning rate.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getMomentum() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{momentum}

\textbf{Parameters: } None

\textbf{Returns: }
The current network momentum.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getInitialWeightMax() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{initialWeightMax}

\textbf{Parameters: } None

\textbf{Returns: }
The initial weight maximum value.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
long getTrainingCycle() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{trainingCycle}

\textbf{Parameters: } None

\textbf{Returns: }
The current training cycle

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
float getAccumulatedInput() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{accumulatedInput}

\textbf{Parameters: } None

\textbf{Returns: }
The current value of the network's accumulated input.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const float * getHiddenNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{hiddenNodes}

\textbf{Parameters: } None

\textbf{Returns: }
A (const) pointer to the array containing the current values of the hidden nodes.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
const float * getOutputNodes() const;
\end{lstlisting}

\textbf{Description: }
Getter for \lstinline{outputNodes}

\textbf{Parameters: } None

\textbf{Returns: }
A (const) pointer to the array containing the current values of the output nodes.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Save/Load API}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
Network_L *loadNetwork(std::string filename);
\end{lstlisting}

\textbf{Description: }
Reads the network configuration found in the file \lstinline{filename}, and initialises a new \lstinline{Network_L} with the configuration.

\textbf{Parameters: } 

\lstinline{filename} The filename of the configuration file to load. 

\textbf{Returns: }
A pointer to the newly initialised \lstinline{Network_L}. If \lstinline{filename} is not a valid configuration file, the function will return \lstinline{Null}

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Signature:} \begin{lstlisting}
int saveNetwork(std::string filename, Network_L *network);
\end{lstlisting}

\textbf{Description: }
Saves the given network's configuration to the given file, if necessary overwriting the existing contents of the file. 

\textbf{Parameters: } 
 
\lstinline{filename} The file to save the configuration to. If the file exists, it will be overwritten.

\lstinline{*network} The network to save the configuration of. 

\textbf{Returns: }
0 on success, or 1 if file saving was unsuccessful.

\hrulefill %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Network Configuration File Format}%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Network configurations will be laid out in such a way that the configuration files are valid C++ header files and can be stored in Arduino program memory through the \lstinline{#include} directive.

To that end, configuration files will be stored as \lstinline{.h} files.

The specification for the configuration file format is as follows:

\begin{itemize}
\item One line containing the text \lstinline{#ifndef ARDUINO_CONFIG_H}
\item One line containing the text \lstinline{#define ARDUINO_CONFIG_H}
\item One blank line
\item One line containing the text \lstinline{#include "avr/pgmspace.h"}
\item One blank line
\item One line containing the text \lstinline{const int numInputNodes = }, followed by the number of input nodes and a semicolon
\item One line containing the text \lstinline{const int numHiddenNodes = }, followed by the number of hidden nodes and a semicolon
\item One line containing the text \lstinline{const int numOutputNodes = }, followed by the number of output nodes and a semicolon
\item One line containing the text \lstinline{const float learningRate = }, followed by the learning rate and a semicolon
\item One line containing the text \lstinline{const float momentum = }, followed by the momentum and a semicolon
\item One line containing the text \lstinline{const float initialWeightMax = }, followed by the initial maximum weight and a semicolon
\item One blank line
\item One line containing the text \lstinline{// TrainingCycle (not needed on Arduino): }, followed by the training cycle and a semicolon
\item One blank line
\item One line containing the text \lstinline|const float hiddenWeights[numInputNodes +1][numHiddenNodes] PROGMEM = {|
\item \lstinline{numInputNodes + 1} lines, each containing \lstinline{numHiddenNodes} weight values, comma separated, enclosed by curly brackets and followed by a comma.
\item One line containing the text \lstinline|};|
\item One blank line
\item One line containing the text \lstinline|const float outputWeights[numHiddenNodes +1][numOutputNodes] PROGMEM = {|
\item \lstinline{numHiddenNodes + 1} lines, each containing \lstinline{numOutputNodes} weight values, comma separated, enclosed by curly brackets and followed by a comma.
\item One line containing the text \lstinline|;|
\item One blank line
\item One line containing the text \lstinline{#endif // ARDUINO_CONFIG_H}
\item One blank line
\end{itemize}

Examples of valid configuration files can be found \todo[inline]{in the appendices (link)}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design \& Specification - Exercise Classifier}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The exercise classifier will be a multi-device system that uses a neural network on a microcontroller to classify exercises the user performs, then sends the classification over Bluetooth to a phone for display. The network will be trained on a desktop computer.

\todo[inline]{diagram?}

\subsection{Hardware}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section details the requirements of the three main pieces of hardware needed to complete the project. It includes both the specifications of the hardware I will use, and the minimum specifications of any alternative hardware.

\subsubsection{Microcontroller}

The requirements of the microcontroller for this project are the most stringent, and are as follows:

\todo[inline]{finish}

\begin{description}
\item[Processor] The microcontroller's processor needs to be capable of performing a classification within 100 milliseconds, to ensure that the user receives feedback on their exercises in a short enough interval to be useful. Given \todo[inline]{the network}, the minimum clock speed is around 211KHz.\cite{dsref0}
\item[Memory] \todo[inline]{work out how much memory the damn thing takes}
\item[Storage] The microcontroller needs to have enough storage for the weights. Given \todo[inline]{the network}, this takes about 17KB not including the bootloader\cite{dsref2}.
\item[Accelerometer] The microcontroller needs to be capable of measuring acceleration on three axes through an IMU or accelerator, either onboard or via a shield.
\item[Bluetooth] The microcontroller needs to be capable of connecting over a Bluetooth Low Energy connection, either onboard or via a shield.
\item[Battery] A battery capable of providing enough current for the chosen microcontroller. The Arduino 101 draws around 70 mA when processing reasonably intensely.\cite{dsref3}
\item[Connectivity] The microcontroller must have some way of connecting to the computer, either over Bluetooth or some form of Serial connnection.
\end{description}

For my implementation, I used an \textbf{Arduino 101} board, which contains an Intel Curie module. It has the following specifications:\cite{dsref4}

\begin{description}
\item[Processor] 32MHz
\item[Memory] 24KB SRAM
\item[Storage] 196KB Flash memory
\item[Accelerometer] 6-axis IMU
\item[Bluetooth] Integrated BLE 
\item[Battery] A standard 9V battery
\item[Connectivity] USB
\end{description}

\todo[inline]{picture}

\subsubsection{Microcontroller Attachment}

To attach the microcontroller to the user's body, it will need some form of attachment. For my implementation, I chose to attach the Arduino to a belt with a wide range of possible lengths. This belt was then passed around the wearer's upper torso and secured. 

\todo[inline]{Link to spec which details where it will be (below)}

\todo[inline]{picture}

\subsubsection{Linux Computer}

The requirements of the computer are not onerous, and most of them are standard on just about all modern computers:

\begin{description}
\item[Processor] In order to train the network effectively, a faster processor than that of the microcontroller is necessary. 
\item[Memory] Likewise, the computer will need enough memory to hold the network. For any network which can be fitted onto the Flash (or other) memory of a microcontroller, this is unlikely to be a problem.
\item[USB Ports] At least one USB port is necessary to upload files to the microcontroller.
\item[Linux] I haven't tested any of the software on other operating systems, so a machine running a Linux variant is probably necessary.
\item[Software] A list of the necessary software can be found in the \todo{Project Management section (link)}
\end{description}

My machine, a 2013 Lenovo IdeaPad running Ubuntu 16.04, fulfills all of the above requirements.

\todo[inline]{picture}

\subsubsection{Android Phone}

Bluetooth Low Energy support was added in Android 4.3 (Jelly Bean MR2), with the Level 18 API.\cite{dsref5} Any phone running Android 4.3 or later, and which supports Bluetooth Low Energy, will suffice for this project.

My phone is a 2015 Huawei Nexus 6, running Android 7.1.2 (Nougat).

\todo[inline]{picture}

\subsection{Microcontroller Software Architecture}%%%%%%%%%%%%%%%%%%%%%%%%

There are four different programs to be run on the microcontroller, each with an accompanying program running on the phone or computer. Each program can be found in a folder of the same name, within the folder \lstinline{curie/src/}.

\subsubsection{logger-supervised}

This program will be used to log example exercises for training when someone is available to supervise. Because of this, the microcontroller program is simple.

After calibration and connecting to the computer over a Serial connection, the microcontroller will sample the data from the accelerometer at 75ms intervals and transmit the raw values over Serial as a space separated, newline terminated string as follows:
\lstinline|xvalue yvalue zvalue\n|

While sending data, the microcontroller will blink an LED to indicate activity.

Because of the simplicity of the program, the entirety of the code will be contained in one file called \lstinline{logger-supervised.ino}

When using this program, the accompanying program \lstinline{log-data-supervised} must be run on the computer.\todo{link to section}

\subsubsection{logger-unsupervised}

This program will be used to log example exercises for training when no one is available to supervise. Because of this, the microcontroller program is slightly more complex than the supervised logger, as it will need to recognise the start and end of movements.

After calibration and connecting to the computer over a Serial connection, the microcontroller will detect when movement begins and send a message over Serial with the following contents: \lstinline{Motion detected after X milliseconds. Logging...} where \lstinline{X} is the elapsed time since the end of the last period of motion or the start of the program.

Once motion is detected, the program will sample the data from the accelerometer at 75ms intervals and transmit the raw values over Serial as a space separated, newline terminated string as follows:
\lstinline|xvalue yvalue zvalue\n|

It will continue to do this until the end of the motion is detected, at which point it will cease sampling and send another message over Serial, this time with the contents \lstinline{Motion ended after Y milliseconds. Logging...} where Y is the elapsed time in milliseconds since motion started.

Because of the simplicity of the program, the entirety of the code will be contained in one file called \lstinline{logger-unsupervised.ino}

When using this program, the accompanying program \lstinline{log-data-unsupervised} must be run on the computer. \todo{link to section}

\subsubsection{classifier-serial}

This program will be used to test the classifier without relying on a Bluetooth connection. The use of a Serial connection also allows for the values of the output nodes to be returned for analysis, rather than just the classification.

After calibration and connecting to the computer over a Serial connection, the microcontroller will detect when movement begins, begin sampling from the accelerometer every 75ms and then save the sum of the raw values to a buffer. When movement ends, or the buffer is full, it will then normalise the data and attempt a classification. After classification, it will send the values of the output nodes over Serial as a space separated, newline terminated list.

Because this program relies on the neural network library, it will be structured as follows:

The main program will reside in \lstinline{classifier-serial.ino}. This will \lstinline{#include} the file \lstinline{network-arduino.hpp}, which will in turn \lstinline{#include} the configuration stored in \lstinline{arduino_config.h}. 
This structure will also implicitly include the implementation file for the network, \lstinline{network-arduino.cpp}.

\todo[inline]{diagram of includes}

When using this program, the accompanying program \lstinline{analyse-classifications} must be run on the computer. \todo{link to section}

\subsubsection{classifier-ble}

This program will be used to perform classifications and send the results to a phone for display, which is the final 'end user' application of the project.

After calibration and connecting to the phone over a Bluetooth Low Energy connection, the microcontroller will detect when movement begins, begin sampling from the accelerometer every 75ms and then save the sum of the raw values to a buffer. When movement ends, or the buffer is full, it will then normalise the data and attempt a classification. After classification, it will send the classification as an integer over BLE to the phone.

Because this program relies on the neural network library, it will be structured as follows:

The main program will reside in \lstinline{classifier-ble.ino}. This will \lstinline{#include} the file \lstinline{network-arduino.hpp}, which will in turn \lstinline{#include} the configuration stored in \lstinline{arduino_config.h}. 
This structure will also implicitly include the implementation file for the network, \lstinline{network-arduino.cpp}.

\todo[inline]{diagram of includes}

When using this program, the accompanying program \lstinline{ble-classifier} must be run on the phone. \todo{link to section}

\subsection{Linux Computer Software Architecture}%%%%%%%%%%%%%%%%%%%%%%%%%

There are a variety of programs for the computer, broadly split into four parts:

\begin{itemize}
\item C++ programs for creating and training networks using the neural network library
\item Python scripts for logging and normalising data ready for training
\item Python scripts to compile the C++ programs and all their dependencies easily. These are noted in the entries for their respective programs.
\item Unit tests, runners and scripts to compile code for unit testing. These are discussed in the Project Management section \todo{link}
\end{itemize}

\subsubsection{new-network}

This program is used to create a new neural network with the configuration specified in the command line arguments, and then save the network configuration to the file with filename specified in the arguments.

The arguments are as follows (in order:

\lstinline{filename} The file in which to store the network configuration. Can be any valid textual file type, but should be a \lstinline{.h} file. If the file already exists, the program will exit without creating the new network configuration.

The other six arguments contain the parameters which control the network configuration, as specified in the \lstinline{Network_L} constructor.

\lstinline{nin} The number of nodes to create in the input layer.

\lstinline{nhn} The number of nodes to create in the hidden layer.

\lstinline{non} The number of nodes to create in the output layer.

\lstinline{lr} The learning rate of the network.

\lstinline{m} The momentum term of the network.

\lstinline{iwm} The initial maximum of the weights in the network

The main code for this program will be contained in the file \lstinline{new-network.cpp}, in the folder \lstinline{linux/src/}. It depends on the two header files \lstinline{network-linux.hpp} and \lstinline{network-saveload-linux.hpp} and implicitly their implementation files \lstinline{network-linux.cpp} and \lstinline{network-saveload-linux.cpp} respectively, all of which are in the folder \lstinline{network/src/}.

\todo[inline]{UML diagram/dependency diagram}

Because of these dependencies, the script \lstinline{compile-new-network} in the folder \lstinline{linux/} will be used to compile the new network program into the executable \lstinline{new-network}, for ease of use.

\subsubsection{train}

This program is used to load a given neural network and then train it on the training data in the given file or directory. The arguments are as follows (in order):

\lstinline{config_filename} The name of the file containing the network configuration. If the file does not exist or is not a valid configuration file, the program will exit.

\lstinline{[-d]} This flag indicates that the program should recursively scan the directory and train the network on every valid normalised log file in the directory. 

\lstinline{dirname|log_filename} If the \lstinline{-d} flag is present, then the third argument is the name of the directory to scan. Otherwise, it is the name of the file containing the data to train on. If the data file is not valid, it will not be used for training.

The main code for this program will be contained in the file \lstinline{train.cpp}, in the folder \lstinline{linux/src/}. It depends on the two header files \lstinline{network-linux.hpp} and \lstinline{network-saveload-linux.hpp} and implicitly their implementation files \lstinline{network-linux.cpp} and \lstinline{network-saveload-linux.cpp} respectively, all of which are in the folder \lstinline{network/src/}. It further depends on the training set library contained in \lstinline{training-set.hpp} and \lstinline{training-set.cpp}, both found in the same folder as the training program, \lstinline{linux/src}

\todo[inline]{UML diagram/dependency diagram}

Because of these dependencies, the script \lstinline{compile-train} in the folder \lstinline{linux/} will be used to compile the training program into the executable \lstinline{train}, for ease of use.

\subsubsection{training-set library}

This code, found in the files \lstinline{training-set.hpp} and \lstinline{training-set.cpp} within the folder \lstinline{linux/src/} define the class \lstinline{TrainingSet} and a single function: \lstinline{TrainingSet *loadTrainingSet(std::string filename);}.

This function takes as an argument a filename, which must refer to a properly normalised log file, and returns a pointer to a new \lstinline{TrainingSet}, which is used by the training program to feed inputs and targets to the neural network.

Keeping this functionality separate enables the precise format of the normalised log files to be decoupled from the logic of training the network.

\subsubsection{log-data-supervised}

This script is used to receive accelerometer data over a Serial connection and save it to a given log file, with start and end markings input by the supervisor.

After opening a serial connection to the microcontroller and opening a file to log to, the program writes all the values it is sent to the log file and to screen, along with additional lines on specific key presses by the supervisor, which are as follows:

\begin{description}
\item[s] \lstinline|Repetition Start\n|
\item[g] \lstinline|Repetition End\n 1\n|
\item[h] \lstinline|Repetition End\n 0\n|
\end{description}

Additionally, on the key press \textbf{x}, the logging script will stop logging and close the file.

The single argument is as follows:

\lstinline{filename} The name of the log file to write to, without the \lstinline{.txt} suffix which will be added by the script. If it already exists, the file will be appended to rather than replaced.

This script is entirely contained in the file \lstinline{log-data-supervised} in the project root directory, and has no dependencies on other project code.

It should be run in conjunction with the \lstinline{logger-supervised} program on the microcontroller.

\subsubsection{log-data-unsupervised}

This script is used to receive accelerometer data over a Serial connection and save it to a given log file, with start and end markings input by the microcontroller.

After opening a serial connection to the microcontroller and opening a file to log to, the program writes all the values it is sent to the log file and to screen, and will exit on any key press.

The single argument is as follows:

\lstinline{filename} The name of the log file to write to, without the \lstinline{.txt} suffix which will be added by the script. If it already exists, the file will be appended to rather than replaced.

This script is entirely contained in the file \lstinline{log-data-unsupervised} in the project root directory, and has no dependencies on other project code.

It should be run in conjunction with the \lstinline{logger-unsupervised} program on the microcontroller.

\subsubsection{normalise-data}

This program is used to transform the 'raw' log files saved by \lstinline{log-data-supervised} and \lstinline{log-data-unsupervised} into a normalised format suitable for training the neural network on. 

It duplicates or removes log entries as necessary until each exercise has the same number of entries as there are nodes in the given input layer, and then saves the sum of the raw values to the normalised log file, along with the exercise delimiters.

A log file with the filename \lstinline{name.txt} will receive the corresponding normalised log file \lstinline{name_normalised.txt}.

The arguments are as follows:

\lstinline{[-d | -r]} If the \lstinline{-d} flag is included, then the given directory should be normalised, while if the \lstinline{-r} is included the given directory and all it's subdirectories should be normalised. Only \lstinline{.txt} files, which don't include the suffix \lstinline{_normalised.txt} in their filename will be normalised.

\lstinline{filename | dirname} If either of the two flags are present then this argument contains the name of the directory to scan, otherwise it contains the name of the file to normalise. If the file is not a \lstinline{.txt} file or the filename contains the \lstinline{_normalised.txt} suffix then it will not be normalised.

\lstinline{readings} The number of readings to normalise to, which must be equal to the number of nodes in the input layer of the network being normalised for.

\lstinline{target} The zero indexed index of the desired target in the output layer of the network being normalised for. 

For example, if the network has three output nodes representing A, B and C respectively, then a value of 1 would write the targets as 010, when normalising a 'good rep', while a value of 2 would write them as 001.

This script is entirely contained in the file \lstinline{normalise-data} in the project root directory, and has no dependencies on other project code.

It should be run on files created by the \lstinline{log-data-supervised} and \lstinline{log-data-unsupervised} scripts.

\subsubsection{monitor}

This program simply opens a Serial connection and prints any input over said connection to the screen. It will exit on any key press.

\subsection{Android Phone Software Architecture}%%%%%%%%%%%%%%%%%%%%%%%%%%

There are only two programs for the phone, both connecting over Bluetooth to the microcontroller.

\subsubsection{BLE-Classifier}

This program displays the classifications made by the microcontroller, and keeps a tally of the number of exercised performed, which is the final 'end user' application of the project.

After connecting to the microcontroller over a Bluetooth Low Energy connection, the program will display the data in the format defined in the section \todo[inline]{Data Display}

As an Android project, the architecture of the program is relatively complex. All of the code for the program resides in the folder \lstinline{android/ble-classifier}. 

\todo[inline]{Sanitise folder and update section. Do diagram}

\subsubsection{BLE-Logger}

This program receives logging data and writes it to a log file along with markers input by the supervisor, in a similar manner to the \lstinline{log-data-supervised} script for computer.

After connecting to the microcontroller over a Bluetooth Low Energy connection, the program will display the the logging data. When the \lstinline{Start Logging} button is pressed, it will begin writing to file, adding the following additional lines on the given button presses:

\begin{description}
\item[Start Repetition] \lstinline|Repetition Start\n|
\item[Finish Repetition (good)] \lstinline|Repetition End\n 1\n|
\item[Finish Repetition (bad)] \lstinline|Repetition End\n 0\n|
\end{description}

The program will finish logging when the \lstinline{Stop Logging} button is pressed.

The filename of the log file will be entered by the user prior to beginning logging.

As an Android project, the architecture of the program is relatively complex. All of the code for the program resides in the folder \lstinline{android/ble-logger}. 

\todo[inline]{Sanitise folder and update section. Do diagram}

\subsection{Data Capture \& Format}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Microcontroller Placement}

In order to ensure that the results are consistent, the microcontroller must always be calibrated and attached in the same way.

Because calibration requires no movement, it is best to place the microcontroller down on a flat surface for calibration, with the top of the board orientated upwards like so

\todo[inline]{picture}

The board must then be attached in the same place for each exercise. This placement can vary as long as it is always the same for the exercise. I chose the following placements:

For the press ups and lunges, on the back between the shoulder blades with the belt tight up under the armpits, and the USB port orientated to the left (the writing on the board is the correct way up). This keeps the microcontroller well out of the way, and prevents the possibility of damaging it while doing press ups.

\todo[inline]{picture}

For the sit ups, because placing it on the back would potentially cause damage, I placed it on the front of the user's torso just below the collarbone, with the belt once again tight up under the armpits and the USB port pointing to the left (as before, the writing should be the correct way up).

\todo[inline]{picture}

\subsubsection{Log File Specification (Supervised)}

A supervised log file will be a \lstinline{.txt} file, and generally be called \lstinline{set}, followed by a numerical suffix, for example \lstinline{set3.txt}. It is recommended that log files be stored in folders such that it is obvious whose exercises have been recorded.

The format of the file will be as follows; zero or more instances of \emph{repetitions}, each consisting of:

\begin{itemize}
\item One or more lines with the values of the inputs.
\item A line with the text \lstinline{Repetition Start}
\item One or more lines with the values of the inputs.
\item A line with the text \lstinline{Repetition End}
\item One or more lines with the values of the outputs, one line per output node
\item One or more lines with the values of the inputs.
\end{itemize}

An example of a valid supervised log file can be found \todo[inline]{todo: link}

\subsubsection{Log File Specification (Unsupervised)}

An unsupervised log file will be a \lstinline{.txt} file, and generally be called \lstinline{set}, followed by a numerical suffix, for example \lstinline{set3.txt}. It is recommended that log files be stored in folders such that it is obvious whose exercises have been recorded.

The format of the file will be as follows; zero or more instances of \emph{repetitions}, each consisting of:

\begin{itemize}
\item A line with the text \lstinline{Motion detected after X milliseconds. Logging...}
\item One or more lines with the values of the inputs.
\item A line with the text \lstinline{Motion ended after Y milliseconds. Logging...}
\end{itemize}

Where X and Y are the intervals between the last motion detection event.

Prior to normalisation, the correct targets will need to be manually added to the log file.

An example of a valid unsupervised log file can be found \todo[inline]{todo: link}

\subsubsection{Normalised Log File Specification}

A normalised log file will be a \lstinline{.txt} file, and have the name of the set from which it was normalised, followed by the suffix \lstinline{_normalised}. For example, the normalised log file for the (raw) log file \lstinline{set1.txt} would be \lstinline{set1_normalised.txt}.

The format of the file will be as follows; zero or more instances of \emph{repetitions}, each consisting of:
\begin{itemize}
\item A line with the text \lstinline{Repetition Start}
\item One or more lines with the values of the inputs, with one line/value per input node. Each value will be equal to the \textbf{sum} of the raw input values
\item A line with the text \lstinline{Repetition End}
\item One or more lines with the values of the targets, with one line/value per output node.
\end{itemize}

There should be no blank lines between repetitions.

An example of a valid repetition can be found \todo[inline]{link}

\subsection{Neural Network Architecture}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The parameters of the neural network for exercise classification will be as follows:

\begin{description}
\item[Input Nodes: ] 20
\item[Hidden Nodes: ] 10
\item[Output Nodes: ] 3
\item[Learning Rate:] 0.3
\item[Momentum: ] 0.9
\item[Initial Weight Max: ] 0.5
\end{description}

\todo[inline]{Update parameters as necessary}

The network parameters were decided upon after experimentation with a variety of values - for more information on how these values were arrived at, see \todo[inline]{section?}

The network will be trained on \todo{number} examples (\todo{breakdown of numbers}), with a further \todo{another number} (\todo{breakdown}) kept back for validation.

For information on how the neural network will work, see the section on the neural network library. \todo[inline]{link}

\subsection{Classification by the Neural Network}%%%%%%%%%%%%%%%%%%%%%%%%%

Having returned a set of output values from the classifier, the following algorithm will be used to classify the result:

\begin{lstlisting}
Given a set of output values vs and a minimum classification threshold t
classificationIndex = -1
for each value v in vs \{
  if v > t \{
      classificationIndex = index(v,vs)
        t = v
    \}
\}
return classificationIndex
\end{lstlisting}

Where \lstinline{index(v,vs)} is a function that returns the index of v in the set vs.

\todo[inline]{include code?}

The classification can then be transmitted to the phone for display (see below).

\subsection{Connectivity}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The microcontroller will communicate over a Bluetooth Low Energy connection with the phone. Upon classification, the microcontroller will send a single integer representing the (zero indexed) index of the output node corresponding to the classification, or the integer \lstinline{-1} if the input was not classifiable (see above for the specifics of the algorithm).

\subsection{Data Display}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In response to the integer transmitted by the microcontroller, the phone will display a message indicating the latest classification. The mapping between the integers and the messages to be displayed is as follows:
\begin{description}
\item[0] "Press Up"
\item[1] "Sit Up"
\item[2] "Lunge"
\item[-1] "Not Valid"
\item[default] "Unknown"
\end{description}

The inclusion of the default "Unknown" is intended to handle errors during transmission or classification.

On updating, the previous message will fade out and the new one fade in. This ensures that even if the same message is displayed twice in a row (a likely occurrence), the arrival of the new classification will be apparent.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Plan and Management}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section briefly details how I managed my project, and the tools and methodologies that I employed. 

Hardware specifications can be found \todo[inline]{in the Design and Spec section (link)}, while an evaluation of how the various tools and methodologies worked in practice can be found \todo[inline]{in the evaluation section (link)}

\subsection{Timetable}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Because the focus of my project changed significantly between the submission of my interim report and the submission of this report, my initial project timetable (as recorded in my interim report) is different to my revised project timetable. The revised project timetable is detailed below, while the initial timetable can be found in the appendices.\todo[inline]{Add initial timetable to appendices and link}

Because the revised timetable was only drawn up after the interim report, it contains no entries before March.

My timetable is organised by 'milestones' - each consisting of a set piece of implementation with a projected date for completion.

\subsubsection{End of research/start of implementation - 4/4/17}

Finish research so as to be ready to start implementation on time.

\subsubsection{Implementation of logging functionality - 14/4/17}

Finish implementation of a logging program to record example data and store it on computer in preparation for training.\todo[inline]{See relevant section and link}

Finishing this functionality early will enable data collection to continue alongside development, so that when I am ready to start training I will have a large enough data set to do so.

\subsubsection{Implementation of C++ neural network library - 28/4/17}

Reimplement ArduinoANN as a C++ library (see \todo[inline]{link to design section}). Additionally, implement a program to train an example of a network on the training data already connected, and do so.

\subsubsection{Implementation of classifier - 5/5/17}

Using the previously implemented neural network library, implement a classifier that runs on the Arduino and sends it's classifications back to the computer over Serial for display.

\subsubsection{Implementation of BLE functionality - 19/5/17}

Extend the classifier to send results over a BLE connection to a phone. Implement a program that runs on the phone and displays classifications sent over BLE from the Arduino.

\subsubsection{Implementation of multiple Curie networking (extension)- 2/6/17}

Train multiple networks to enable classification together, and adapt the classifier as necessary.
Extend the phone program to accept multiple BLE connections and display the results.

This milestone also acts as a buffer, giving an extra two weeks if the core implementation overruns without eating into the time to write the report.

\subsubsection{Report submission - 19/6/17}

Write the project report, and evaluate the classifier performance. 

\subsubsection{Presentation - 26/6/17}

Prepare for the project presentation, fixing bugs as necessary to make a demonstration work. 

\subsubsection{Project archive submission - 3/7/17}

Tidy up codebase and write user guide. Fix any remaining bugs and make programs more robust.

\subsection{Version Control}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{git}

I chose git\cite{ppref0} for my version control system, largely because of my prior experience with it. Within git, I used a variation of a standard workflow known as \textit{gitflow}.\cite{ppref1} 

I kept the convention of having separate \lstinline{master} and \lstinline{development} branches, but did not use release branches. 

In addition to these two branches, and the feature branches, I used two more long term branches:

The branch \lstinline{data} was created once the logging functionality was complete, and was used to track changes to the loggers and to track the recording of data. This branch was then merged to \lstinline{development} whenever necessary to make the data available for other branches.

The branch \lstinline{documentation} was created at the very start, and contains only documentation, in particular the project diary (\todo[inline]{link}) and both reports. While the projects were both written in Overleaf\todo[inline]{link}, the changes were transferred to the branch regularly.

\subsubsection{Github}

Much like git, I chose to use Github\cite{ppref2} to host my git repository largely because of my familiarity with it. 

I maintained a repository\cite{ppref3} for the duration of the project and stored virtually everything relating to my project in it.

\subsection{Languages \& Libraries Used}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The multi platform nature of this project meant that I used several languages for the various constituent parts/

\subsubsection{C++}

The core neural network code (for both the Linux and Arduino versions, see \todo[inline]{link} for details) was written in C++\cite{ppref4}. I also wrote the training and save/load code in C++.

I chose to follow the \lstinline{c++11}\cite{ppref5} standard to enable me to make use of the new \lstinline{std::vector<>} that was introduced with \lstinline{c++11}.

I used the C++ unit testing library \textit{Catch}\cite{ppref6} to write unit tests for my C++ code.

\subsubsection{Arduino}

While software for the Arduino can be written in any language which has a compiler capable of compiling a binary for the Arduino's processor, most Arduino programming is done in the Arduino C/C++ variant\cite{ppref7}, which takes file suffix \lstinline{.ino}. This variant adds some Arduino specific libraries\cite{ppref8}, but lacks many of the normal C/C++ libraries - most notably the C++ Standard Template Library (STL).\cite{ppref9}

The main Arduino programs were written in Arduino C++. I used the Intel libraries \textit{CurieIMU}\cite{ppref10} and \textit{CurieBLE}\cite{ppref11} to provide access to the Curie's IMU and BLE adapter respectively.

\subsubsection{Python}

I used Python\cite{ppref12} to write all of the necessary scripts for logging data on the computer, normalising data, and for compiling and running automated unit tests.

I used Python 2.7.12 for this project, but could have just as easily done it in Python 3.

Alongside the core Python modules \textit{sys, serial, os, subprocess} and \textit{random}, I made use of the \textit{curses}\cite{ppref13} module for terminal control.

\subsubsection{Java (Android)}

Android\cite{ppref14} programs are written in Java, which is then compiled for the Android Runtime (ART)\cite{ppref15}, instead of normal Java bytecode.

I wrote the Android phone programs to display data in Java.

\subsection{Tools Used}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{GCC}

All versions of the Gnu Compiler Collection (GCC) since GCC 4.8.1 have supported the C++11 standard\cite{ppref16}. I chose to use version 6.3.0\cite{ppref17} to compile all C++ code that runs on my computer.

\subsubsection{AVR-GCC}

To compile C/C++ code for the Arduino requires the AVR-GCC compiler. This comes bundled with the Arduino IDE\cite{ppref18} (see below).

All versions of the Arduino IDE since 1.6.6 have enabled C++11 by default.

\subsubsection{Arduino IDE}

Aside from allowing the user to edit code, the Arduino IDE\cite{ppref18} manages compilation and upload of programs to a connected Arduino.

I used version 1.8.2, but any version since 1.6.6 would have worked just as well.

\subsubsection{CLion}

I did most of my programming in the CLion\cite{ppref19} IDE. I used version 2016.3.1 

\subsection{Android Studio}

I used Android Studio\cite{ppref20} to edit my Java code, and to manage compilation and upload of the Android code. I used version 2.3.2.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

My project was broadly split into two major contributions: the implementation of a C++ neural network library, and the implementation of an exercise classifier using the library. For both of these contributions, I managed to complete a complete (if not fully featured) implementation.

Reviewing the project contributions (as defined in the Introduction \todo{link}), the specific list of individual contributions is as follows:

\begin{itemize}
\item Neural Network Library 
  \begin{itemize}
  \item Implementation of a neural network capable of running on  both desktop and microcontroller
    \item Storage and retrieval of network configurations
  \end{itemize}
\item Exercise Classifier
  \begin{itemize}
  \item Programs to acquire training data
    \item A trained network capable of reliably classifying the three chosen exercises
    \item An implementation of a classifier using the above network, running on a microcontroller
    \item An implementation of the exercise classifier making use of more than one microcontroller (extension)
  \end{itemize}
\item A data set to train the exercise classifier on
\end{itemize}

\subsection{Neural Network Library Contributions}%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Implementation of a neural network capable of running on  both desktop and microcontroller}

The original design for the neural network library called for a a single header and implementation that could be included on either a desktop or embedded system. The library worked well on desktop and with the inclusion of an Arduino port of of C++ STL\cite{evref1} I managed to get the library to run on the Arduino. 

Unfortunately despite the theoretical memory usage of the network being less than the RAM available on my Arduino system, I was unable to instantiate a network of the size required due to memory fragmentation. 

To solve this I renamed the original \lstinline{Network} class \lstinline{Network_L} and created a second, stripped down, class called \lstinline{Network_A} that would be much less memory intensive.

I achieved this by removing the training functionality. This allowed me not only to remove all those vectors and code used solely in the training process, but enabled me to make the weights and parameters \lstinline{const} and store them in the Arduino's program memory instead of RAM. 

The lack of training functionality, while not much of a problem in practice, is a major blow because it is a key piece of functionality.The switch to separate 'desktop' and 'microcontroller' versions of the same library also makes maintaining the library more difficult.

Approaches to solving these problems are discussed in the Future Work section \todo{link}.

\subsubsection{Storage and retrieval of network configurations}

The original plan for storage and retrieval of network configurations called for a \lstinline{.txt} file which could be read simply on both the embedded and desktop systems. The changing of the design into two separate classes did little to change this.

The issues with the Arduino implementation of the neural network (see above) made the completion of this contribution easier rather than harder. With the requirement that training on the Arduino be possible removed, it became possible to store the network parameters and weights in program memory by writing them as constants into a header file and including it at compile time. 

This did increase the complexity of the parser for loading configurations on desktop, but only slightly, and did not affect the functionality of the desktop neural network at all.

The main downsides of this approach are that it restricts the microcontroller to running only a single neural network at a time, and that the library code is very tightly coupled to the configuration file (indeed it will not compile without it). While the former is unlikely to be much of an issue in practice, it is disappointing not to have made a more general approach work.
The latter is an issue not because it compromises any of the functionality, but because it will make it much more difficult to change the behaviour of the library in future. 

Approaches to solving these problems are discussed in the Future Work section \todo{link}.

\subsubsection{Neural Network Evaluation Metrics}

In the introduction\todo{link}, I defined some evaluation metrics for the neural network as follows:

The library must be able to do the following:

\begin{itemize}
\item Create an arbitrarily sized network (with one hidden layer)
\item Run on both desktop and embedded systems
\item Be capable of storing and retrieving configurations from disk
\item Be capable of implementing all the functionality of ArduinoANN
\end{itemize}

From the above, all of the capability metrics have been met, with the partial exception of the second metric (run on both desktop and embedded systems).

As discussed above, the library can run on both desktop and embedded systems, but it does so via two versions of the library code, and can only train the network in one of them.

If the library is efficiently implemented, it will:

\begin{itemize}
\item It must use as little memory as possible
\item It must use as little processing power as possible
\item It must be as easy to use as possible
\item It must have as few dependencies as possible
\end{itemize}

For the above efficiency metrics, once again they have been met to a reasonable degree. 

The memory usage is very low, mainly because of the lack of training capability. This means that only the values of the nodes must be kept in memory, using X \todo{input actual number}B for my exercise classifier example with 20 input nodes, 10 hidden nodes and one output node. 

Processing power is also very low - my exercise classifier performs classifications in an average of 658 microseconds\cite{dsref0}. 

The ease of use is discussed at length in combination with the ease of use for the exercise classifer later in the evaluation.\todo{link}

The library really only depends on the version of C++ (C++11 or greater) and the C++ STL, both of which are widely available. 

\subsection{Exercise Classifier Contributions}%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Programs to acquire training data}

The original design called for three programs to help record training data:

\todo[inline]{link back to design section?}

\begin{description}
\item[logger-supervised] For data collection when another person was available to manually delimit and mark exercises
\item[logger-unsupervised] To automatically delimit and record exercises when no one was available to supervise
\item[logger-ble] For data collection without needing to be physically connected to a desktop computer
\end{description}

\textbf{logger-unsupervised} presented something of a chicken and egg problem - how do you detect when an exercise starts and ends without knowing what an exercise looks like?

My approach made use of the motion detection features in the Curie IMU - 
While this mostly worked well, it imposed a fairly restrictive style of recording on the user (fast movements so as to ensure that the sensor did not detect zero motion before the exercise was finished, and long pauses in between repetitions to ensure that zero motion was detected after completion of a repetition, along with the necessity of watching the computer screen to check that the repetitions were being recorded accurately and frequent breaks to note which repetitions of a set should be marked 'good' or not) which made it very frustrating to try and record large volumes of data.

To deal with this problem, I decided to implement a second logger that made use of a person watching the user and marking their exercises in real time.

\textbf{logger-supervised} was successfully implemented soon after the problems with \lstinline{logger-unsupervised} became apparent, I switched to almost exclusively using the supervised logger and I continued to use it to record data right up until the end of the project. As a simple program, it performed well and did everything I needed it to do.

The only real issue with this program was that it relied on a Serial (USB) connection to my computer, limiting mobility. To address this issue, I decided to implement a third logger that would send the logging data over a Bluetooth Low Energy connection to a phone, allowing me to log data without being tethered to a laptop.

\textbf{logger-ble} was my attempt to solve the issue of being tethered to a laptop when recording data. I attempted an implementation, but was unable to get even a rudimentary version working in time to record any data with it.

The issue was with the BLE Notification system on Android - I was unable to enable notifications, and as such was unable to properly receive new accelerometer values for logging (note that I later fixed this issue enough to implement the BLE classifier).

Although not itself a core contribution, this failure to deliver a working system (the only outright failure to deliver a contribution in this project) had a negative impact on the quantity of data I was able to collect, as discussed in the Data Collection section. \todo{link}

\subsubsection{A trained network capable of reliably classifying the three chosen exercises}

Using the training data that I was able to collect and the neural network library, I determined the optimal parameters for the network and trained it on the data.

An in depth look at the performance of this network can be found in the Classifier Performance section of the evaluation\todo{link}, while the process of finding the optimal parameters is detailed 

\subsubsection{An implementation of a classifier using the above network, running on a microcontroller}

I implemented two versions of the microcontroller-based classifier: \lstinline{classifier-serial}, which connects over Serial and I mainly used for testing the classifier, and \lstinline{classifier-ble}, the 'full' version sending data over a BLE connection to a phone.

\textbf{classifier-serial} was fairly simple to implement, drawing on the motion detection that I tested in the \lstinline{logger-unsupervised} program and the neural network library.

The motion detection still retains the flaws detailed earlier\todo{link} - motion detection is temperamental and can force the user to be quite robotic in their movements.

\textbf{classifier-ble} was much more difficult to make work, mostly because of the difficulties presented by the Android BLE Notification system noted earlier\todo{link}. 
I was surprised to find that working with BLE on the Arduino was much simpler than working with it on Android. Having got this working, the rest of the system was a simple UI for display, which works well. 

\subsubsection{Exercise Classifier Evaluation Metrics}

In the introduction\todo{link}, I defined some evaluation metrics for the exercise classifier as follows:

The classifier must be able to do the following:

\begin{itemize}
\item Classify the user's movements \textit{in real time}, or close to it
\item Connect over Bluetooth to a phone
\item Display the classifications
\end{itemize}

All three of these metrics have been met. The classifier takes on average 658 microseconds\cite{dsref0} to perform a classification, and the Arduino reliably connects to the phone and displays classifications.

If the classifier is efficiently implemented, it will:

\begin{itemize}
\item It must classify the movements reliably and correctly
\item It must use as little memory as possible
\item It must use as little processing power as possible
\item It must be as easy to use as possible
\end{itemize}

Classifier performance is discussed at length in the section below (Classifier Performance\todo{link}), while ease of use is discussed together with the library in the section Ease of Use \todo{link}.

The memory usage is very low because the library stores the weights in program memory. This means that only the values of the nodes must be kept in RAM, using X \todo{input actual number}B for the network and Y B \todo{actual number} overall when running (the rest of the memory being used for the BLE functionality).

Processing power is also very low - as previously stated, my exercise classifier performs classifications in an average of 658 microseconds\cite{dsref0}. 

\subsection{Classifier Performance}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{All the normal classification measures. Will probably need numbers for all the versions. Note how library constrained performance too (IE wrong error/activation functions)}

\subsection{Energy Consumption}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{How long did the battery last? Is this practicable for the smaller form factor Curie etc}

\subsection{Ease of Use}%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Neural Network Library}

Using the neural network library is quite simple. The programmer simply needs to download the source files and include them in their project wherever needed. This should be familiar to just about all programmers.

Where the library becomes slightly more difficult is using it on the microcontroller. The library is very tightly coupled to the configuration file and so must be used in precisely the right way in order to work. Attempting to use the library in unexpected ways may therefore prove more difficult than expected.

\subsubsection{Exercise Classifier}

There are two potential cases to look at as far as ease of use is concerned. The first is the person who set up the classifier and prepares it for classification. The second is the person who actually performs the exercises and uses the classifier. These may be the same person, but this is not required.

Setting up the classifier requires the following steps:

\begin{itemize}
\item Download the repository from Github
\item Open the Arduino IDE and open the \lstinline{classifier-ble} file
\item Connect the Arduino and upload
\item Open Android Studio and open the \lstinline{ble-classifier} project
\item Connect the phone and upload
\end{itemize}

Since the person setting up the classifier is likely to be a developer of some description, this seems reasonably easy. In order for the classifier to be used more widely it would be necessary to simplify this process, but as it stands this is not an issue.

Using the classifier is conceptually simple; the steps are as follows:

\begin{itemize}
\item With the classifier running on the Arduino, open the application on the phone
\item Connect to the device
\item Do exercises, and the classifications should appear
\end{itemize}

In practice there are two problems with this. Firstly, the BLE notifications don't always work and so it may be necessary to manually enable them (as discussed in the section???\todo{link}).

The second issue is the unreliable motion detection, which can force the user to do their exercises in a somewhat robotic manner. 

Ideas for how to fix these two issues are discussed in the Future Work section. \todo{link}

\subsection{Data Set Contribution}

It is more or less axiomatic that in the world of machine learning, you can never have too much data. My aim was to record between 500 and 1000 examples of each of the three exercises, with a roughly 3:1 ratio between positive and negative examples, for a total of 3000 example repetitions. This meant that the total number of examples would be equal for each of the four classifications (Press Up, Sit Up, Lunge \& None)

My results were as follows:

\begin{center}
  \begin{tabular}{|c|c|c|c|}
      \hline
      Exercise & Target & Actual & Percentage \\
      \hline
      Press Up + & 750 & & \\
      \hline
      Press Up - & 250 & & \\
      \hline
      Sit Up + & 750 & & \\
      \hline
      Sit Up - & 250 & & \\
      \hline
      Lunge + & 750 & & \\
      \hline
      Lunge - & 250 & & \\
      \hline
      Total & 3000 & & \\
      \hline
      \end{tabular}
\end{center}

It is obvious from the above table that I struggled to get the amount of data that I wanted. Fortunately, the data I did manage to gather seems to have been sufficient (see the section above on Classifier Performance)\todo[inline]{link. Change this if performance was unsatisfactory}

Nonetheless, it is still worthwhile asking why I failed to record as much data as I originally intended. 

The primary reason that I recorded less data than intended is that press ups, sit ups and lunges are physically taxing.\cite{evref2} This limited me to recording around 50 exercises on any given day, even without other forms of exercise, and putting a hard limit of around 2700 on how many exercises I was able to record.\cite{evref3} 
Naturally, recording hundreds of examples of exercises made me fitter and so towards the end of the project I was able to record more examples per day, particularly when I was recording multiple different exercises together rather than one type, but this increase was more than balanced out by the fact that I was only able to record data four\todo{correct?} times a week on average.

To increase the quantity of data collected, and to prevent the network from overfitting to my actions, I attempted to gather data from other people. 

Firstly I recorded some data from friends and family, in total recording X examples \todo{input number}. This helped somewhat, but was never going to allow me to collect hundreds of examples in a day, so I decided to try and collect data from a larger group.

Initially, I went to the Department of Computing labs and tried to bribe DoC students to do press ups with sweets. This went about as well as expected, and resulted in no additional data. 

My other solution was to bring the Arduino to a rugby training session and borrow my teammates for a few minutes each to record exercises for me. Unfortunately this idea was thwarted by my inability to make the BLE logger work (see section on contributions\todo{link}), but I am confident that this would have solved my remaining data collection issues.

\subsubsection{Data Set Evaluation Metrics}

In the introduction\todo{link}, I defined some evaluation metrics for the exercise classifier as follows:

The data set must have the following:

\begin{itemize}
\item Correctly labelled data for all of the classifications
\item Data of the correct format for consumption by the neural network
\end{itemize}

Both of these metrics have been met. I met the first by using supervised learning and manually checking that the targets for each repetition were correct.

The second was achieved through my normalising script, which ensured that each repetition had the same number of data points as the number of input nodes.

If the data set is efficient, it will:

\begin{itemize}
\item It must have as little noise as possible
\item It must have as few misclassified examples as possible
\item It must have the correct balance of examples so as to maximise the efficiency of the classifier
\item It must have enough examples to train the classifier effectively
\end{itemize}

The amount of noise in the dataset is difficult to evaluate. Certainly my normalising approach (dropping or duplicating data points at random) increased the noise, but the calibration methods and consistency of data collection have reduced the noise as far as practicable.

I believe that my reliance on supervised exercises using the \lstinline{logger-supervised} program, rather than my unsupervised version has kept the number of misclassified examples low, although this is of course difficult to enumerate.

The final two metrics are discussed in the section Classifier Performance\todo{link}.

\subsection{Project Management}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Overall, my project management during the project was more of a help than a hindrance. My choices of version control systems, languages and tools caused me very little in the way of real issues, while the inclusion of slippage time in my timetable meant that I was able to deliver the core contributions despite my overrunning on them.

\subsubsection{Timetable}

My timetabling was the weakest part of my project management. The table below details my estimated and actual finish dates for each milestone, along with my estimated and actual time taken to complete each milestone and the percentages of that time.

\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|}
      \hline
      Milestone & Estimated Finish & Actual Finish & Estimated Time & Actual Time & \% \\
      \hline
      Logging functionality & 14/4 & 14/4 & 10 & 10 & 100 \\
      \hline
      Neural network library & 28/4 & 5/5 & 14 & 21 & 150 \\
      \hline
      Classifier & 5/5 & 26/5 & 7 & 21 & 300 \\
      \hline
      BLE functionality & 19/5 & 5/6 & 14 & 8 & 57\\
      \hline
      Extension & 2/6 & - & 14 & - & - \\
      \hline
      Report submission & 19/6 & 19/6 & 16 & 12 & 75\\
      \hline
      \end{tabular}
\end{center}

From the table, it's I took longer than expected to reach my milestones and as a result had to cut my extension tasks in order to hand in the report on time.

The reasons for this vary on a case by case basis but a common theme is thinking that tasks would be easier than they were, caused by my lack of experience working with embedded systems.

The initial milestone (logging functionality) went exactly to time, but this result hides the fact that I later had to spend time implementing a different logger (see section on contributions for more information \todo{link}), which was only finished on the 26/4 and so took time away from the neural network library.

The second milestone also started well; it took only two days to create a C++ library from the ArduinoANN code, and the rest of the week to finish writing and testing the core network library. The save/load functionality progressed slowly but steadily, and by the end of the milestone I was apparently only a week behind schedule.

This illusion was comprehensively shattered when implementing the classifier. Although I had tested the network code extensively on my Linux machine, I hadn't tested it on the Arduino and it would not compile.

Initially I misdiagnosed the error as being due to the difference between certain constructs in C++11 and previous versions of C++, and spend two days trying to fix this before realising that the implementation of the C++ STL available on the Arduino didn't include key necessary features (\lstinline{std::vector, std::random_device} and \lstinline{std::string}. I managed to overcome this problem with an external library, and by the 12/5 had managed to make the network run on the classifier, if not work properly.

It was at this point that I encountered the lack of memory problem, and decided that I needed two versions of the network library. Rewriting the network library took until the 23/5, at which point the classifier itself took only three days. 

From the above it's clear that my estimate for the amount of work to implement the classifier was relatively accurate, but that said estimate relied on a working library which I did not have. My mistakes implementing the neural network library put me three weeks behind schedule, and it was at this point that I decided to abandon the extension completely and get a minimum viable implementation of the BLE classifer running, giving myself until 6/6 to do so.

In actuality, the implementation of the BLE functionality on the microcontroller took less than a day, and most of the time was spent wrestling with the Android portion of the program. I managed to get the minimum viable implementation running on the 5/6, and started work on the report on the 7/6, only four days behind schedule.

\todo[inline]{Finish writing this section when report is almost done}

Overall, while my naivety regarding the complexities of implementing embedded software caused most of my project to overrun, my inclusion of a two week extension/buffer at the end meant that I was able to complete the core contributions of my project successfully. 

For future projects I will be careful to factor in time taken to debug things that are harder than they seem, and make sure that I continue to include a buffer at the end of the project.

\subsubsection{Version Control}

In general my use of git and Gitflow worked extremely well. It helped me keep the various aspects of the project separate and at no point did I lose data or have non working code on the \lstinline{development} or \lstinline{master} branches. 

I did notice that I had to manually sync my \lstinline{.gitignore} files across branches, as work done on one branch often added code that needed ignoring in others, but this was problem consumed a couple of minutes a week and never lead to missing data.

\subsubsection{Languages \& Libraries Used}

Most of the language choices in the project were made for me - Arduino programming must be done in Arduino C/C++ and Android programming is almost exclusively in Java. 

With that said, I am of the opinion that C/C++ was a good choice of language to program the neural network in. C++11 gives access to useful features (most notably vectors), while the language still retains access to lower level functionality that proved vital on the Arduino (most notably the PROGMEM attribute).

The choice of Catch to unit test my code proved a good one. I ended up with 464 test assertions across my codebase, and this undoubtedly saved me a large amount of debugging time. Catch allowed for easy writing of unit tests, and only once did it not have a feature I required (assertions cannot be made against numbers defined using the \lstinline{#define} macro).

I feel that my choice of Python to do my scripting in (rather than Bash or another option) was the correct one. Python is powerful and easy to use - it enabled me to quickly write scripts to do many of the mundane tasks required and so concentrate on the more difficult aspects of my project. 

\subsubsection{Tools Used}

I had no real issues with GCC (at one point I thought I was getting errors due to the version, but this was proved false), likewise with AVR-GCC.

The Arduino IDE managed the upload of files well, but I found the editing side to be lacking in features. Of particular annoyance was the fact that it doesn't track changes to files on disk, which caused some issues when switching branches with the IDE still open. Whenever possible, I wrote code in CLion instead.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Conclusion}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The twin goals of this project were to develop a neural network library for embedded systems, and then to implement an exercise classifier using the library. Both of these goals have been met, albeit with some caveats, so I consider the project to be a qualified success.

I implemented the neural network library as a C++ class, allowing it to be easily included in projects. The biggest challenge was overcoming the lack of RAM in embedded systems, which I (partially) overcame through the use of program memory instead. 

I implemented the exercise classifier as an Arduino C++ program running on the Arduino 101, and communicating over a Bluetooth Low Energy connection with an Android application written in Java. 
Surprisingly, the biggest challenge for this implementation was the BLE connection; the library made the machine learning section of the implementation very simple.
Unfortunately the difficulties encountered during implementation of the neural network library made it impossible for me to complete my extension work using multiple Arduino 101s.

Through the implementations above I proved that it is possible to run artificial neural networks on microcontrollers, and made it easier for others to do so.

\subsection{Future Work}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While I have completed what I believe to be minimal viable implementations for both of these contributions, there are many things that could be done to make them more useful.

\subsubsection{Fixing Project Deficiencies}

Over the course of the project I encountered several issues that made my implementations of the library and classifier less capable than I would wish. The first thing to do as part of any future work would be to fix these issues.

The only major issue with the neural network library is that it cannot be trained on any system that does not have enough RAM to hold the entire network, and as such the network must exist at compile time.

The simplest solution is of course to acquire more RAM. The standalone Curie module comes with 80KB of SRAM\cite{fwref2}, which is considerably more than the 24KB of the Arduino 101. This approach is effective, but is liable to be more expensive and is not always an option.

Another solution to this would be to store those parts of the network related to training (weights, and the deltas of those weights) in program memory, of which there is often more than RAM, and rewrite them as necessary. This is possible using the \lstinline{avr/boot.h} library, but requires writing whole pages at a time and writing to program memory is frowned upon to say the least.\cite{fwref0}

The Arduino PROGMEM library also does not support the storage of vectors in program memory, probably because vectors are not part of the Arduino standard libraries.

A better solution would be to store data in EEPROM, which is much easier to do and has none of the potential issues of writing to program memory.\cite{fwref1} Unfortunately, the Arduino 101 has only 1KB of EEPROM and so external EEPROM (or a different embedded system) would be necessary.

Writing to external EEPROM is more work than writing to internal EEPROM or RAM because the two main interfaces, SPI and I2C won't handle reading and writing objects, so the programmer must handle this themselves. Nonetheless, this solution almost completely solves the issue of lack of memory, and additionally provides non volatile storage of the network configuration.

Both the additional RAM and EEPROM solutions also mean that the network can be constructed using vectors rather than arrays, which makes it much easier to create networks whose dimensions are not known at compile time. 

Another problem caused by the lack of memory of microcontrollers, is the splitting of the library into two implementations. While the non-RAM solutions above would mean that the two implementations of the library would have the same capabilities, it would still require two implementations.

Solving this issue is only possible if the library can either keep everything in RAM and have a 'bolt on' module for non volatile storage (the same function that the separate file \lstinline{network-saveload-linux} performs) or the library can detect which platform it is being run on and adjust where it puts the data accordingly. 

A major usability issue with the current exercise classifier is the motion detection's unreliability. The most straightforward approach to this would be to make the motion detection more intelligent. 

As an example, when doing press ups the person will often briefly stop moving at the bottom of the repetition, before pushing themselves back up. With naive motion detection, the this is often registered as the end of the movement, and so classification is attempted on the first half of the move and subsequently returns a negative result.
If the zero motion detection is sufficiently insensitive to skip this brief period of immobility, then the user must stay motionless for longer at the end of the movement before this lack of motion is registered.

\todo[inline]{Picture explaining problem}

A more intelligent motion detection method might recognise the zero motion at the bottom of the move and attempt classification, but continue logging from the start of the move and attempt classification of the whole move (that is, both the up and down movements) after the user returns to the up position.

\todo[inline]{picture explaining solution}

However, this approach still suffers from the fundamental chicken and egg problem of not knowing what to attempt classification on.

Another way to combat this issue would be to maintain a queue of the last 30 or so readings and attempt classification every time a new reading (or if this proves too computationally expensive, several readings) is added. 
This would return a lot of negative classifications, but would mean that no valid exercises were missed, and would stop the user from having to adapt their movements to suit the motion detector. This would have the drawback of using more power, but would mean that the only thing attempting to decide what is and isn't an exercise is the neural network rather than a more primitive system.

\todo[inline]{This figure shows a simplified 10-second sample containing two exercises (marked in green). In this period there would be several negative classifications (marked in red).}

\subsubsection{Neural Network Library}

The implementation of the neural network library is fairly limited in it's capabilities. Once the training issues have been fixed, there are several fairly obvious improvements that could be made.

One of the simplest changes would be to implement different activation functions. Currently the library uses the sigmoid function, but it would be simple or even trivial to implement alternatives such as the linear, hyperbolic tangent and SoftMax functions.

Similarly, the network uses the Sum Squared error function, and it would be simple to implement alternatives such as the Cross-Entropy function or Exponential function. These additional error functions would be necessary in order to use some of the different activation functions. 

A bigger change would be support for an arbitrary number of hidden layers. The current implementation is hard coded to a single hidden layer, and while this is sufficient most of the time, it would be useful to support as many layers as is necessary. 
This would be a substantial change in the internal workings of the library, but would mean little change to the API.

Likewise, other architecture changes, such as partial connectivity and convolutional networks would be possible to implement, and given the dominance of the weights in memory usage (the number of weights in a fully connected network being roughly the square of the number of nodes) would be useful to keep the number of weights and therefore memory usage down.

While the library does support changing the momentum and learning rate, it currently has no way of easily configuring these to change (for example, to halve the learning rate every \textit{N} epochs). This is more of an ease of use change than a missing feature, but it would be good to add.

The library currently uses the on-line backpropagation algorithm, wherein weights are updated after every example. It would be trivial to implement batch learning (where the weights are updated after several examples). 

The library currently lacks any mechanism for regularisation. Dropout, a popular mechanism, would be difficult to implement as it would require changing weights and the number of neurons mid execution, but other techniques such as L1 and L2 regularisation perform much the same function without modifying the network topology. It is worth noting though that dropout reduces the size of the network, which can only be a good thing on an embedded system.

Finally, alternatives to backpropagation for gradient descent could be considered. This would be a fairly major change, and is not one I feel qualified to talk intelligently about.

In principle, any existing training technique is a potential candidate for addition to the library; what separates good candidates from bad candidates is firstly how useful they would be, and secondly how much more or less efficient they would make the library - given that the main constraint on the network is memory rather than processor speed, things that decrease training time are comparatively less important.

With this in mind, I would prioritise adding the different activation and error functions, as they would be easy to implement, make the library usable across several different classes of problem and would not have a noticeable effect on the efficiency of the library.

I would then implement the functionality for arbitrary numbers of layers. Although I cannot see most neural networks for embedded systems having more than two or possibly three hidden layers, being limited to one hidden layer is a major drawback of the current library. 

After that, I would consider implementing regularisation as it is the other major missing feature. Batch learning would also be a good candidate for inclusion, given it's simplicity, but is less important.

\subsubsection{Exercise Classifier}

One of the main incentives for using the Curie module, and the focus of my extension milestone that I did not manage to undertake, was the idea of using multiple Curies to send much more detailed IMU data for classification.

The Curie module is available as a button sized (11x8mm)system on a chip.\cite{fwref2} With several of these placed at strategic locations on the body, extremely accurate IMU data could be used not only to classify motions, but to analyse them and provide detailed feedback.

As an example, a system with a Curie between the shoulder blades, on one knee and on a buttock could not only tell you if you had done a press up, but tell you if your core was properly rigid or if your knees were sagging during the exercise. Likewise, the addition of a curie on the elbow could tell you if you were keeping your arms close in to your body or splaying them.

This would shift the burden of classification from the modules themselves onto the system linking them together (such as a phone), but the modules could do some of the computation for themselves - they could do the 'dead reckoning' to turn their raw IMU readings into a movement and send the result to the overall classifier.

This technology would be useful for any application wherein a very precise knowledge of \textit{how} a person (or other object) has moved and the forces they have undergone is desired.

To pick an example close to my own interests, rugby players could use such a system (with modules embedded in a scrum cap and shirt) to flag up potential concussions and notify match officials that they may need treatment or to leave the field.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix 1: User Guide}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Installation}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{How to: install all the stuff. Ideally git repo + single script to install dependencies should be enough}

\subsection{Setup}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{How to get ready to classify (inc. training the network to match your movements, probably. If not this section becomes less useful).}

\subsection{Usage}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{How to: actually use the thing}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix 2: Examples and Code Samples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Example Log Files}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Supervised Log File Example}

An example of a valid supervised log file is as follows (this example contains two repetitions, and there are two output nodes - the correct target is the first of the two):

\begin{lstlisting}
2112 3521 9697
 -820 4980 8498
 4928 1414 12078
 2182 2625 17658
Repetition Start
 420 6706 17109
 390 4711 18996
 34 4158 16245
 112 3384 18010
 -122 988 16876
 1132 1626 18761
 133 2256 17444
 1329 758 17024
 162 1943 14781
 260 2308 14217
 438 3703 14671
 1522 3522 8614
Repetition End
1
0
 267 1716 11535
 2021 2455 11427
 1941 2974 9327
 1187 3642 12921
 1484 624 13763
Repetition Start
 2301 4362 15325
 2448 786 14240
 1014 3284 17920
 1094 4471 17259
 3094 623 18312
 315 3334 16080
 1351 1072 20128
 2352 564 16417
 2270 66 17622
 1286 238 15131
 2303 2767 13679
 2078 2553 13658
 3072 3089 11358
 -1023 2758 10359
Repetition End
1
0
 2286 1498 10641
 1187 3017 11019
 938 2236 10331
\end{lstlisting}

\subsubsection{Unsupervised Log File Example}

An example of a valid unsupervised log file is as follows (this example contains one valid repetition and an unfinished one, which will be removed in the normalisation process):

\begin{lstlisting}
Motion detected after  794  milliseconds. Logging...
-3645 258 18409
 -1192 -1167 21047
 -2562 -3021 25236
 -2985 -4357 29204
 -1892 -6278 24810
 -1823 -4548 25039
 -347 -4235 22112
 -1465 -5232 24840
 -1546 -5415 26162
 -1849 -4882 22686
 -3693 -5339 22833
 -2114 -4549 21613
 -2126 -6574 21918
 -2303 -4939 20830
 -1795 -3754 19054
 -209 -2626 19330
 506 -460 19227
 874 -1628 17378
 -3168 -813 18150
 -4758 -501 17496
 595 1684 15219
 976 -1727 15472
 264 -629 19411
 Motion ended after  3018  milliseconds. Logging...
Motion detected after  784  milliseconds. Logging...
-2141 1791 16200
 -395 -237 17426
 -2397 164 19727
 -1568 -1193 22452
 -1612 -4823 26304
 -2882 -6720 26317
 -2768 -6063 24776
 -2561 -4488 23331
 -1143 -3689 23230
 -2256 -5549 23404
 -823 -3498 23925
 -1676 -6678 26284
\end{lstlisting}

\subsubsection{Normalised Log File Example}

An example of a valid repetition for a network with 20 input nodes and one output node is as follows:

\begin{lstlisting}
Repetition start
20119
20119
23526
25971
30233
33248
33248
34300
29982
29982
27556
25647
22664
17422
17422
17422
15862
15862
16160
18608
Repetition end
1
\end{lstlisting}

\subsection{Example Network Configuration Files}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Untrained Network}

\todo[inline]{Complete this}

\subsection{Trained Network}

This example shows a network that has undergone training. As such, the \lstinline{TrainingCycle} counter has increased, and some weights are now larger than the defined \lstinline{initialWeightMax}.

\begin{lstlisting}
#ifndef ARDUINO_CONFIG_H
#define ARDUINO_CONFIG_H

#include "avr/pgmspace.h"

const int numInputNodes = 20;
const int numHiddenNodes = 10;
const int numOutputNodes = 1;
const float learningRate = 0.300000;
const float momentum = 0.900000;
const float initialWeightMax = 0.500000;

// TrainingCycle (not needed on Arduino): 317

const float hiddenWeights[numInputNodes +1][numHiddenNodes] PROGMEM = {
    { -0.208936, 0.465415, -0.051608, 0.266630, 0.244777, -0.345782, 0.473185, 0.130999, -0.512141, 0.541225 }, 
    { -0.184242, -0.185961, 0.048651, -0.447796, -0.403301, -0.377087, -0.363783, -0.344564, 0.268100, 0.156894 }, 
    { -0.438798, -0.231550, -0.432171, 0.417060, 0.295765, 0.006337, 0.381011, -0.045423, 0.253946, -0.132465 }, 
    { 0.157203, -0.181294, -0.175319, 0.237207, 0.400133, -0.268108, -0.034001, 0.321978, -0.561890, 0.015838 }, 
    { 0.095073, -0.414909, 0.395242, 0.111011, 0.269341, -0.449858, 0.206365, 0.524708, -0.181234, -0.351355 }, 
    { -0.466847, -0.126106, 0.147103, -0.334612, -0.083822, -0.268722, -0.237934, -0.125875, -0.332713, -0.008082 }, 
    { 0.123222, -0.198338, -0.274432, 0.513173, 0.302451, -0.456392, 0.008966, 0.280207, -0.410660, 0.301137 }, 
    { -0.513978, 0.149379, -0.238859, -0.288574, 0.239729, -0.248091, 0.330426, -0.257049, 0.216124, 0.568632 }, 
    { -0.367217, -0.258441, -0.506060, -0.255655, -0.499114, 0.113885, 0.188771, 0.014348, 0.213099, -0.113877 }, 
    { -0.480785, -0.364806, -0.396680, 0.497976, -0.502551, 0.128596, -0.261498, 0.462252, -0.598261, 0.253812 }, 
    { -0.567159, 0.236277, 0.162137, -0.427903, 0.198453, -0.061932, -0.172801, 0.418995, -0.272858, -0.358574 }, 
    { 0.159899, -0.399958, 0.233211, -0.149129, 0.289593, -0.324470, 0.244713, 0.587779, 0.016516, 0.539349 }, 
    { 0.129768, 0.396204, 0.135740, 0.012989, -0.305828, -0.443811, -0.277537, -0.153022, -0.337330, 0.496737 }, 
    { -0.569687, 0.106501, -0.409859, -0.446173, -0.044025, 0.183276, 0.356145, 0.125618, -0.659715, 0.259530 }, 
    { -0.560354, -0.289071, 0.103752, -0.370594, 0.240134, -0.449281, 0.337401, 0.220689, -0.067533, -0.147390 }, 
    { -0.059864, -0.357426, 0.268295, -0.025435, 0.287175, -0.130794, -0.345558, -0.056022, -0.592401, 0.409725 }, 
    { 0.070506, 0.494606, 0.012306, 0.027240, -0.112442, 0.128436, -0.409556, 0.127022, 0.280491, 0.565825 }, 
    { 0.368003, -0.237027, -0.127455, -0.135200, 0.429812, -0.477968, 0.417062, 0.129507, -0.251470, -0.289104 }, 
    { 0.244408, -0.091057, -0.426142, 0.103974, -0.089830, 0.466919, -0.213426, -0.357478, -0.044970, -0.003307 }, 
    { -0.186364, -0.420361, 0.170605, 0.392511, 0.417764, -0.235246, 0.305627, -0.136452, -0.280800, 0.048744 }, 
    { -0.090482, -0.197617, 0.347492, -0.188813, 0.381779, -0.349393, -0.213947, -0.088926, -0.425888, 0.317125 }, 
};

const float outputWeights[numHiddenNodes +1][numOutputNodes] PROGMEM = {
    { 0.033657 }, 
    { 0.187269 }, 
    { 0.944105 }, 
    { 0.532517 }, 
    { 1.497310 }, 
    { 0.390718 }, 
    { 0.715542 }, 
    { 0.833513 }, 
    { 0.245622 }, 
    { 1.482894 }, 
    { 1.962036 }, 
};

#endif // ARDUINO_CONFIG_H
\end{lstlisting}


\subsection{Code Samples}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{Code samples?}

\todo[inline]{Add additional things}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}

% Introduction references (inref#)

\bibitem{inref1}
\url{http://webarchive.nationalarchives.gov.uk/20170110165405/http://www.noo.org.uk/NOO_about_obesity/adult_obesity/UK_prevalence_and_trends}
\textbf{Public Health England / UK Government}
UK and Ireland obesity prevalence and trends
\\\textit{Accessed 7/6/2017}

% Background references (bgref#) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibitem{bgref0}
\url{https://ibug.doc.ic.ac.uk/courses}
\textbf{Imperial College London, Department of Computing}
CO395 Machine Learning course home page
\\\textit{Accessed 7/6/2017}

\bibitem{bgref1}
\url{http://people.sabanciuniv.edu/berrin/cs512/reading/mao-NN-tutorial.pdf}
\textbf{A.K Jain, J. Mao \& K. Mohiuddin, Michigan Stage University}
Artificial Neural Networks: A Tutorial
\\\textit{Accessed 7/6/2017}

\bibitem{bgref2}
\url{http://www.glyn.dk/download/Synopsis.pdf}
\textbf{F. Nelson}
Neural Networks - Algorithms and Applications
\\\textit{Accessed 7/6/2017}

\bibitem{bgref3}
\url{http://robotics.hobbizine.com/arduinoann.html}
\textbf{Author Unknown, Hobbizine}
A Neural Network for Arduino
\\\textit{Accessed 7/6/2017}

\bibitem{bgref4}
\url{http://www.cs.bham.ac.uk/~jxb/INC/nn.html}
\textbf{J.A. Bullinaria, School of Computer Science, University of Birmingham}
John Bullinaria's Step by Step Guide to Implementing a Neural Network in C
\\\textit{Accessed 7/6/2017}

\bibitem{bgref5}
Looking specifically at backpropagation, D.E. Rumelhart, G.E. Hinton and R.J. Williams were working on back propagation in neural networks in 1986 (see \url{https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf}), although much of their work was based on the earlier work of others, in particular Paul Werbos. 
The original Apple Macintosh, released in 1984, had a 7.83MHz processor and 128KB of RAM. (source: \url{http://oldcomputers.net/macintosh.html}, \textit{accessed 7/6/2017}
). For comparison, the Arduino 101 I am using runs at 32MHz, and 196KB of Flash memory, although it only has 24KB of SRAM. (source: \url{https://www.arduino.cc/en/Main/ArduinoBoard101}, \textit{accessed 7/6/2017}).

If devices such as smartphones are considered, the superiority in power is obvious.

\bibitem{bgref6}
\url{https://www-ssl.intel.com/content/www/us/en/wearables/wearable-soc.html}
\textbf{Intel}
Intel Curie Module home page
\\\textit{Accessed 7/6/2017}

\bibitem{bgref7}
\url{https://www.bluetooth.com/specifications/bluetooth-core-specification}
\textbf{Bluetooth}
Bluetooth core specification
\\\textit{Accessed 7/6/2017}

\bibitem{bgref8}
\url{https://www.fitbit.com/uk/smarttrack}
\textbf{Fitbit}
Fitbit SmartTrack Auto Exercise Recognition
\\\textit{Accessed 7/6/2017}

\bibitem{bgref9}
\url{https://www.android.com/intl/en_uk/wear/}
\textbf{Google}
Android Wear home page
\\\textit{Accessed 7/6/2017}

\bibitem{bgref10}
\url{https://www.google.com/fit/}
\textbf{Google}
Google Fit home page
\\\textit{Accessed 7/6/2017}

\bibitem{bgref11}
\url{http://optimize.fitness}
\textbf{Optimize Fitness}
Optimize Fitness home page
\\\textit{Accessed 7/6/2017}

\bibitem{bgref12}
\url{https://boltt.com/}
\textbf{Boltt}
Boltt home page
\\\textit{Accessed 7/6/2017}

\bibitem{bgref13}
\url{http://www.actofit.com/}
\textbf{Actofit Wearables}
Actofit home page
\\\textit{Accessed 7/6/2017}

\bibitem{bgref14}
\url{https://www.indiegogo.com/projects/actofit-redefining-fitness-tracking--3#/}
\textbf{Indiegogo}
Indiegogo campaign page for Actofit
\\\textit{Accessed 7/6/2017}

\bibitem{bgref15}
\url{http://focusmotion.io/}
\textbf{Focus Ventures}
FocusMotion home page
\\\textit{Accessed 7/6/2017}

\bibitem{bgref16}
\url{https://www.nerdfitness.com/blog/proper-push-up/}
\textbf{S. Kamb, Nerd Fitness}
How to do a Proper Push Up
\\\textit{Accessed 7/6/2017}

\bibitem{bgref17}
\url{https://breakingmuscle.com/learn/pimp-your-push-up-3-common-mistakes-and-5-challenging-variations}
\textbf{N. Tumminello, Breaking Muscle}
Pimp Your Push Up: 3 Common Mistakes And 5 Challenging Variations
\\\textit{Accessed 7/6/2017}

\bibitem{bgref18}
\url{https://www.youtube.com/watch?v=Eh00_rniF8E}
\textbf{S. Malin}
How to Do a Push Up Correctly
\\\textit{Accessed 7/6/2017}

\bibitem{bgref19}
\url{https://www.livestrong.com/article/487008-how-to-do-a-correct-sit-up/}
\textbf{A. Cespedes, Livestrong.com}
How to Do a Correct Sit-Up
\\\textit{Accessed 7/6/2017}

\bibitem{bgref20}
\url{http://www.military.com/military-fitness/fitness-test-prep/proper-technique-for-curl-ups}
\textbf{S. Smith, Military.com}
The Proper Technique for Curl-ups
\\\textit{Accessed 7/6/2017}

\bibitem{bgref21}
\url{http://www.mensfitness.com/weight-loss/burn-fat-fast/situp}
\textbf{N. Green, Men's Fitness}
The Situp
\\\textit{Accessed 7/6/2017}

\bibitem{bgref22}
\url{https://www.livestrong.com/article/539595-how-to-do-sit-ups-without-anchoring-your-feet/}
\textbf{A. Cespedes, Livestrong.com}
How to Do Sit-Ups Without Anchoring Your Feet
\\\textit{Accessed 7/6/2017}

\bibitem{bgref23}
\url{https://www.youtube.com/watch?v=jDwoBqPH0jk}
\textbf{M. Tapper, Howcast}
How to Do a Sit-Up Properly
\\\textit{Accessed 7/6/2017}

\bibitem{bgref24}
\url{http://www.shape.com/fitness/workouts/know-your-basics-how-do-lunge}
\textbf{POPSUGAR Fitness}
Know Your Basics: How to Do a Lunge
\\\textit{Accessed 7/6/2017}

\bibitem{bgref25}
\url{https://www.youtube.com/watch?v=jzbXc2OmRMk}
\textbf{30 Day Fitness Challenges}
How To Do The lunge Exercise
\\\textit{Accessed 7/6/2017}

% Neural Network library references (nnref#) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibitem{nnref0}
\url{https://isocpp.org/wiki/faq/cpp11}
\textbf{Standard C++ Foundation}
C++11 Overview and FAQ
\\\textit{Accessed 8/6/17}

\bibitem{nnref1}
\url{http://www.nongnu.org/avr-libc/user-manual/group__avr__pgmspace.html}
\textbf{avr-libc contributors (see \url{http://www.nongnu.org/avr-libc/user-manual/index.html})}
Program Space Utilities
\\\textit{Accessed 12/6/17}


% Design and Specification references (dsref#) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibitem{dsref0}
Figure calculated as follows: On an Arduino 101 (32MHz clock speed), classification took an average of 658 microseconds, so the minimum clock speed necessary is given by 32 * 0.658 / 100 = 211 KHz

\bibitem{dsref1}
Calculations for memory usage

\bibitem{dsref2}
The 'BareMinimum' example sketch uses 48432B of program memory. The classifier sketch uses 65456B of program memory, a difference of 17033B.

\bibitem{dsref3}
\url{https://tlextrait.svbtle.com/arduino-power-consumption-compared}
\textbf{T. Lextrait}
Arduino: Power Consumption Compared
\\\textit{Accessed 11/6/17}

\bibitem{dsref4}
\url{https://www-ssl.intel.com/content/dam/support/us/en/documents/boardsandkits/curie/intel-curie-module-datasheet.pdf}
\textbf{Intel}
Intel Curie Module Datasheet
\\\textit{Accessed 10/6/17}

\bibitem{dsref5}
\url{https://developer.android.com/about/versions/android-4.3.html#Wireless}
\textbf{Google}
Android 4.3 API Guide
\\\textit{Accessed 10/6/17}

% Project Plan and Management references (ppref#)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibitem{ppref0}
\url{https://git-scm.com/}
\textbf{Git }
Git home page
\\\textit{Accessed 8/6/17}

\bibitem{ppref1}
\url{https://www.atlassian.com/git/tutorials/comparing-workflows#gitflow-workflow}
\textbf{Author unknown, Atlassian}
Comparing Workflows
\\textit{Accessed 8/6/17}

\bibitem{ppref2}
\url{https://github.com/}
\textbf{Github Inc}
Github home page
\\\textit{Accessed 8/6/17}

\bibitem{ppref3}
\url{https://github.com/walrus/walrus}
\textbf{D. Clay ('walrus')}
WALRUS (project) Github repository
\\\textit{Accessed 8/6/17}

\bibitem{ppref4}
\url{https://isocpp.org/}
\textbf{Standard C++ Foundation}
Isocpp home page
\\\textit{Accessed 8/6/17}

\bibitem{ppref5}
\url{https://isocpp.org/wiki/faq/cpp11}
\textbf{Standard C++ Foundation}
C++11 Overview and FAQ
\\\textit{Accessed 8/6/17}

\bibitem{ppref6}
\url{https://github.com/philsquared/Catch}
\textbf{P. Nash ('philsquared')}
Catch Github repository
\\\textit{Accessed 8/6/17}

\bibitem{ppref7}
\url{https://www.arduino.cc/en/Reference/HomePage}
\textbf{Arduino AG}
Arduino Language Reference
\\\textit{Accessed 8/6/17}

\bibitem{ppref8}
\url{https://www.arduino.cc/en/Hacking/BuildProcess}
\textbf{Arduino AG}
Arduino Build Process
\\\textit{Accessed 8/6/17}

\bibitem{ppref9}
\url{http://www.arduinolibraries.info/types/official}
\textbf{Arduino Library List}
Official Libraries
\\\textit{Accessed 8/6/17}

\bibitem{ppref10}
\url{https://www.arduino.cc/en/Reference/CurieIMU}
\textbf{Arduino AG / Intel}
CurieIMU Reference
\\\textit{Accessed 8/6/17}

\bibitem{ppref11}
\url{https://www.arduino.cc/en/Reference/CurieBLE}
\textbf{Arduino AG / Intel}
CurieBLE Reference
\\\textit{Accessed 8/6/17}

\bibitem{ppref12}
\url{https://www.python.org/}
\textbf{The Python Software Foundation}
Python home page
\\\textit{Accessed 8/6/17}

\bibitem{ppref13}
\url{https://docs.python.org/2/library/curses.html#module-curses}
\textbf{The Python Softwaer Foundation}
Curses documentation home
\\\textit{Accessed 8/6/17}

\bibitem{ppref14}
\url{https://developer.android.com/guide/index.html}
\textbf{Google}
Introduction to Android
\\\textit{Accessed 8/6/17}

\bibitem{ppref15}
\url{https://source.android.com/devices/tech/dalvik/}
\textbf{Google}
ART and Dalvik
\\\textit{Accessed 8/6/17}

\bibitem{ppref16}
\url{https://gcc.gnu.org/projects/cxx-status.html#cxx11}
\textbf{The GNU Project}
C++11 Support in GCC
\\\textit{Accessed 8/6/17}

\bibitem{ppref17}
\url{https://gcc.gnu.org/gcc-6/}
\textbf{The GNU Project}
GCC 6 Release Series
\\\textit{Accessed 8/6/17}

\bibitem{ppref18}
\url{https://www.arduino.cc/en/Main/Software}
\textbf{Arduino AG}
Arduino main software page
\\\textit{Accessed 8/6/17}

\bibitem{ppref19}
\url{https://www.jetbrains.com/clion/}
\textbf{Jetbrains}
CLion: A cross platform IDE for C and C++
\\\textit{Accessed 15/6/17}

\bibitem{ppref20}
\url{https://developer.android.com/studio/index.html}
\textbf{Google}
Android Studio home
\\\textit{Accessed 15/6/17}

% Evaluation references (evref#) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibitem{evref1}
\url{https://github.com/mike-matera/ArduinoSTL}
\textbf{M. Matera ('mike-matera')}
ArduinoSTL repository root
\\\textit{Accessed 15/6/17}

\bibitem{evref2}
\textbf{D. Clay}
I should know, I've just done X of them. \todo{number}

\bibitem{evref3}
\textbf{D. Clay}
There are 54 days between the 26th of April, when I finished the supervised logging program, and the 19th of June, when the report is due. At 50 exercises per day, every day, that equates to 2700 exercises.

% Conclusions and future work references (fwref#) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibitem{fwref0}
\url{http://www.nongnu.org/avr-libc/user-manual/group__avr__boot.html}
\textbf{Free Software Foundation}
AVR Bootloader Support Utilities
\\\textit{Accessed 16/6/17}

\bibitem{fwref1}
\url{https://www.arduino.cc/en/Reference/EEPROM}
\textbf{Arduino AG}
EEProm Library home
\\\textit{Accessed 16/6/17}

\bibitem{fwref2}
\url{https://www.intel.co.uk/content/www/uk/en/products/boards-kits/curie/module.html}
\textbf{Intel}
Intel Curie Module
\\\textit{Accessed 16/6/17}

\end{thebibliography}

\end{document}
